{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjNf0Z37tzMJ"
      },
      "source": [
        "### To Do:\n",
        "- divide into separate sections, one colab per section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfpC0WOhvYEY"
      },
      "source": [
        "# The 39 Steps\n",
        "### A DS To Do List\n",
        "1. State Goals\n",
        "\n",
        "2. Import (main) libraries\n",
        "- document what all of your software versions are\n",
        "- maybe create a bundle of that software\n",
        "\n",
        "3. Get files/data sets/ etc\n",
        "\n",
        "4. create and setup environment and begin organizing libraries packages and dependencies\n",
        "\n",
        "5. employ unit-tests and check \n",
        "especially for\n",
        "- array size\n",
        "- data type\n",
        "\n",
        "\n",
        "4. Initial Exploration: patterns, issues\n",
        "  - did it load properly, header etc.\n",
        "  - shape\n",
        "  - NaN\n",
        "  - odd characters\n",
        "  - cardinality\n",
        "  - redundancy\n",
        "  - empty columns\n",
        "  - formating of columns\n",
        "\n",
        "5. Features and Focus\n",
        "  - What is Y\n",
        "  - What X features to include (/exclude)\n",
        "  - note time-series for splits\n",
        "\n",
        "6. Establish A Baseline: Results to compare and score your model's performance\n",
        "  - For ordinal, continuous: e.g. mean, median, mode, etc.\n",
        "  - For categorical: \n",
        "  majority \"class\" (\"class\" is a catagory)\n",
        "  - y_train.value_counts()[:1]\n",
        "  - (some say) Start with a kitchen sink \"baseline model\"\n",
        "\"kitchen sink\" model as baseline is usually a basic model like a regression with 'all' features.\n",
        "\n",
        "7. get baseline \"score\":\n",
        "pick score:\n",
        "accuracy, (majority class mean), etc.\n",
        "  \n",
        "8. Splits: Make Train, Val, & Test sets\n",
        "  - issue: random or time based split\n",
        "  - issue: split ratio\n",
        "(for cross validation:\n",
        "randomized_searchcv,\n",
        "(not-preferred-gridsearchcv)\n",
        "(also,bayes-search?(library?)\n",
        "\n",
        "Explore:\n",
        "- pandas profile\n",
        "- \n",
        "\n",
        "\n",
        "9. Wrangle via Function\n",
        "(for all: train,val,test)\n",
        "Wrangle:\n",
        "- duplicates\n",
        "- outliers\n",
        "- \n",
        "\n",
        "10. (Make) Family of Variables\n",
        "\n",
        "11. Deploy Model(s) \n",
        "\n",
        "\n",
        "\n",
        "Before doing pipeline: \n",
        "do a model without the pipeline with each step separate for debugging. \n",
        "\n",
        "NaN: \n",
        " - Some have a NaN policy \n",
        " - \n",
        "\n",
        "via pipeline\n",
        " - Trying models in pipeline:\n",
        " -  linear models\n",
        " -  trees (and ensambles)\n",
        "\n",
        "\n",
        "12. Evaluate Compare, Refine, Redeploy Models\n",
        "- confusion_matrix+\n",
        "- PDP\n",
        "- Shapely\n",
        "- \n",
        "\n",
        "\n",
        "13. Export Test Results (using whole dataset to train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lnvYdjAsq4V"
      },
      "source": [
        "## improved random seed generation\n",
        "\n",
        "https://colab.research.google.com/drive/1iHN61T3AGy4vdg4su5V5g_ElMFWGbL0T?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1VC2ExQll3c"
      },
      "source": [
        "df['author'] = df['author'].map({'Austen':1, 'CBronte':0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVcWtjp7sJOV"
      },
      "source": [
        "Add More For:\n",
        "- cross tabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oJ1QoeIvdnf"
      },
      "source": [
        "#(Title) General Template for Date Science Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMa_SBGvv4Gn"
      },
      "source": [
        "(enter Overall Goal and description of code)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6v-_tfLwIXx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PefGAe-bwI3B"
      },
      "source": [
        "#getting information about items\n",
        "?ttest_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT_rF0Bmzp5o"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTpK8DDNdGhU"
      },
      "source": [
        "#method for organizing and installing libraries et all\n",
        "pip install -r requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1nHUfwadXOQ"
      },
      "source": [
        "#or ?\n",
        "pipenv install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLF0NfWfdn_k"
      },
      "source": [
        "#or ? but not in base env\n",
        "conda install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmUgofCcvd8X"
      },
      "source": [
        "#most common library Imports Set\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHzLnO4JhE45"
      },
      "source": [
        "#more common Import Libraries Set\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import ttest_ind, ttest_ind_from_stats, ttest_rel, t, ttest_1samp\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4vVgBsGzvgH"
      },
      "source": [
        "###Settings and Adjustments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8_QIFWey1uI"
      },
      "source": [
        "#so more head displays\n",
        "#override display option\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui_LeBUMAZWV"
      },
      "source": [
        "#alt, put in length of DF collums\n",
        "#?pd.set_option('display.max_rows',len(df.columns))\n",
        "pd.set_option('display.max_columns',len(df.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wNWh87t4aNG"
      },
      "source": [
        "#tells you version of libraries and packages\n",
        "pd.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78pmQzWN4hjO"
      },
      "source": [
        "#installs earlier version software\n",
        "#!pip install pandas==0.23.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX_HmG73CviV"
      },
      "source": [
        "import joblib\n",
        "import sklearn\n",
        "import category_encoders as ce\n",
        "print(f'joblib=={joblib.__version__}')\n",
        "print(f'scikit-learn=={sklearn.__version__}')\n",
        "print(f'category_encoders=={ce.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LKhSQiZ8GYv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRaKLO2L0fow"
      },
      "source": [
        "#check/set/recheck your local directory\n",
        "\n",
        "import os\n",
        "os.getcwd()\n",
        "#os.chdir('C:\\\\Users\\\\linea\\\\DS8_LS\\\\edu')\n",
        "#os.getcwd()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9M4keLS0HAN"
      },
      "source": [
        "!ls *.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45u87b5R0J8F"
      },
      "source": [
        "#long notation? - human readable\n",
        "!ls -lh *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qc_hR7P6xeP"
      },
      "source": [
        "#shows all libraries and versions as preinstalled\n",
        "#use pip freeze to view all of the \n",
        "#librarires/packages that are installed\n",
        "#in our instance\n",
        "\n",
        "#!pip freeze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXMhtiUEz7po"
      },
      "source": [
        "#change directory\n",
        "%cd instacart_2017_05_01/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmaj4dBFy7z3"
      },
      "source": [
        "##Import Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIVMVeES_OBT"
      },
      "source": [
        "###Before you get your data:\n",
        "check that it is a raw data-file and not another kind of file (e.g. an html page about that file. Look! Look! Look!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtcWE3oX97gr"
      },
      "source": [
        "#inspect file\n",
        "#before loading file, check to see what the file is by loading it into a browser, text editor, or the following:\n",
        "#inspect dataset\n",
        "#e.g.\n",
        "!curl https://..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIHc1GKD91T9"
      },
      "source": [
        "# for colab\n",
        "# run this cell, you will be prompted to upload, click on choose file, \n",
        "# then drag and drop the file in\n",
        "# upload file from local drives\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!ls #check file is there\n",
        "# add that file name to the below and run that\n",
        "df = pd.read_csv('file_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXl2gVhUQWCv"
      },
      "source": [
        "#looking at a few columns\n",
        "train[['longitude', 'latitude']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-jWYJCRUX8"
      },
      "source": [
        "# from local computer (jupyter notebook)\n",
        "df = pd.read_csv(r\"C:\\Users\\linea\\Downloads\\data\\Asteroid data\\asteroid_albedo_1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu7IK7eK0y_g"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsFYEKTly6o-"
      },
      "source": [
        "#e.g. for webscraping\n",
        "#method1: \"standard accepted method\"\n",
        "import requests\n",
        "url1 = 'https://...'\n",
        "response = requests.get(url1)\n",
        "#check to see (validate) that it worked\n",
        "response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK3KBrhK43JY"
      },
      "source": [
        "#Import Method 2\n",
        "!wget https://..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7m9U6mq4vH3"
      },
      "source": [
        "#to unzip a zipped file\n",
        "!unzip filename.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTguA16ObGR4"
      },
      "source": [
        "#unpack a tar\n",
        "!tar --gunzip --extract --verbose --file=instacart_online_grocery_shopping_2017_05_01.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWuvdrKx0qVf"
      },
      "source": [
        "#Check that file exists in local directory\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcON4y3jdPL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNl79cWcjns2"
      },
      "source": [
        "https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmdGdQ-RjffD"
      },
      "source": [
        "# import data from Google Drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzvqVLIDkIUJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87MDewXtjnL1"
      },
      "source": [
        "path = \"copied path\"\n",
        "df_bonus = pd.read_csv(path)# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "#e.g.\n",
        "#/content/drive/My Drive/Projects/Asteroid Projects/asteroid_albedo_5_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8Vcg0WF3Jpp"
      },
      "source": [
        "##Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkWnUfG1zE5-"
      },
      "source": [
        "#Types of Files and Formats\n",
        "#Loading data into Data Frame (df)\n",
        "\n",
        "#Basic loading for csv,tsv,json\n",
        "#CSV setting datasets to a 'df' variable:\n",
        "df1 = pd.read_csv('file_name.csv')\n",
        "\n",
        "#TSV\n",
        "#df3 = pd.read_csv(('FastFmProfiles.tsv'), delimiter='\\t')\n",
        "\n",
        "#JSON\n",
        "#df2 = pd.read_json('PeopleInSpaceNow.json')\n",
        "\n",
        "#What would be the bracket method of doing this?\n",
        "\n",
        "#future add\n",
        "#xml\n",
        "\n",
        "#future add\n",
        "#rss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B1cSro-cfBH"
      },
      "source": [
        "# Here is an example of setting X and y from a dataframe\n",
        "\n",
        "# putting the data into a dateframe\n",
        "df = pd.DataFrame.from_dict(data_going_in).astype('int')\n",
        "\n",
        "# setting X from the dataframe\n",
        "X = df[['x1', 'x2']].values\n",
        "\n",
        "# setting y from the dataframe\n",
        "y = df['y'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax1RgVntvwF-"
      },
      "source": [
        "##Setting Header"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtrphayXvwO9"
      },
      "source": [
        "#header=None (specifies that you will add your own header)\n",
        "\n",
        "#e.g. from UCI site, headings may be listed in text on HTML page, cut and paste those...\n",
        "\n",
        "df = pd.read_csv('URL', header=None, index_col=None, names=header_names)\n",
        "\n",
        "header_names = header_names.split('\\n')\n",
        "header_names\n",
        "#list comprehension\n",
        "#\n",
        "[name.split(':')[0] for name in header_name[1:-1]]\n",
        "\n",
        "df.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiPiOPMyzLz4"
      },
      "source": [
        "\n",
        "##Initial Data Inspection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4viaAS7TGhw"
      },
      "source": [
        "Super Profile Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TVd2GE_TITy"
      },
      "source": [
        "data = train\n",
        "\n",
        "from pandas.plotting import scatter_matrix\n",
        "scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAPDrpxWTJCK"
      },
      "source": [
        "#Pandas Profiling Pandas Profile\n",
        "#holey moley\n",
        "#veras inspect\n",
        "import pandas_profiling\n",
        "pandas_profiling.ProfileReport(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E12qwlHmTfEo"
      },
      "source": [
        "import pandas_profiling\n",
        "#df.profile_report()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWEEWjUW9JQr"
      },
      "source": [
        "# maybe requires older version of pandas, e.g. 0.25.0\n",
        "# or? \n",
        "profile =ProfileReport(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kGRxYYdHl7v"
      },
      "source": [
        "# inspection: outputs names of files\n",
        "data0_files = os.listdir('./data/forest')\n",
        "data1_files = os.listdir('./data/mountain')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PUm4VTUAzd2"
      },
      "source": [
        "#Basic Data Inspections\n",
        "\n",
        "#(default) first five rows\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLlsPTvyAzhs"
      },
      "source": [
        "# shows footer if there is one\n",
        "print (df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UteRPztIAzk5"
      },
      "source": [
        "#alt method\n",
        "#!head file.csv\n",
        "#!tail file.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI909PZNrFA1"
      },
      "source": [
        "https://www.geeksforgeeks.org/how-to-get-column-names-in-pandas-dataframe/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg-9iVwSqiiG"
      },
      "source": [
        "# view collumns\n",
        "df.collumns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf2LC0kAqh5P"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_IlBjF6AznV"
      },
      "source": [
        "#checks for and summarizes missing data\n",
        "print(df.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_BUQCzfAzsE"
      },
      "source": [
        "#prints a complete boolean of missing data (warning this may be long)\n",
        "df.isna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIEP1Q1lu3HM"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGKt8pZYu4mE"
      },
      "source": [
        "df.isnull().sum(). sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks1qqJFsAzur"
      },
      "source": [
        "#How many items are in each column\n",
        "print(df.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c2q1gBMAzqK"
      },
      "source": [
        "#Gives basic statistics\n",
        "# Describes the Numeric Collumns\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NueouS6rFfd3"
      },
      "source": [
        "# This tells you how many different non-numeric variables there are, e.g. if they are binary or nearly so.\n",
        "# different summary\n",
        "# Describes non-numeric collumns\n",
        "df.describe(exclude='number')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcPwxP3eA7i_"
      },
      "source": [
        "#Rows and Columns\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvvnBhpbA7mC"
      },
      "source": [
        "#look at datatypes\n",
        "print(df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kfAYLniA7oM"
      },
      "source": [
        "#view a list of df columns\n",
        "list(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXzztx857ja"
      },
      "source": [
        "# Advanced Inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gfFTBpRorIY"
      },
      "source": [
        "salaries quiz udemy gga\n",
        "https://colab.research.google.com/drive/1349ImiEcmwUtE_2g4AAwsNhnEMH0uxmD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK8GQK8J-B_o"
      },
      "source": [
        "df.iloc[df['TotalPayBenefits'].argmax()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isI8diPS9mIS"
      },
      "source": [
        "#passing a boolean search into a df produces the results...\n",
        "df[df['TotalPayBenefits'] == df['TotalPayBenefits'].max]\n",
        "#same as\n",
        "df.iloc[df['TotalPayBenefits'].argmax()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9TTgBS07sRC"
      },
      "source": [
        "df[df['EmployeeName'] == 'JOSEPH DRISCOLL']['JobTitle']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz_beQRKpKow"
      },
      "source": [
        "# getting mean sorted by column values, e.g. by year\n",
        "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\n",
        "df.groupby(['Year']).mean\n",
        "#or, for just one column\n",
        "df.groupby('Year').mean()['BasePay']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCecBkIr_W72"
      },
      "source": [
        "\n",
        "# view methods that can be used in pandas with dataframes df\n",
        "dir(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7NB2YQj6HB5"
      },
      "source": [
        "df.describe(exclude='number').T.sort_values(by='unique')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJX477LPxZX9"
      },
      "source": [
        "#Alternate way of using .describe - transposes output\n",
        "df.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSkml6zqvflu"
      },
      "source": [
        "# how many items are there with a given value in one column\n",
        "len(df[df['Language']=='en'])\n",
        "#or\n",
        "df[df['Language']=='en'].count()\n",
        "#or\n",
        "df[df['Language']=='en']['Language'].count()\n",
        "#or\n",
        "df[df['Language']=='en'].info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dz509Y26Gjc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyatunH7pz2D"
      },
      "source": [
        "# number of unique values\n",
        "df['JobTitle'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRit6YCypwoX"
      },
      "source": [
        "# list unique values\n",
        "df['JobTitle'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npo89SxI_-uj"
      },
      "source": [
        "len(df['JobTitle'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UgcElttNqtE"
      },
      "source": [
        "df.value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuG-x3ms_8vw"
      },
      "source": [
        "df['JobTitle'].value_counts().head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSfcFlBiNsna"
      },
      "source": [
        "\n",
        "df.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PjlhJ0KBLZZ"
      },
      "source": [
        "# how many specific things have only one value count\n",
        "sum(df[df['Year']==2013]['JobTitle'].value_counts() == 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If7NyVre7N_w"
      },
      "source": [
        "assert 1 == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pnqf02K4F7"
      },
      "source": [
        "#\n",
        "# getting the unique characters in a list\n",
        "chars = sorted(list(set(NAME_OF_LIST)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNpXg3OQFjj_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaEjtQVo2-PT"
      },
      "source": [
        "# contains a string has a string\n",
        "df[df['JobTitle'].str.contains(\"Chief\")].info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HJnKeyn78T"
      },
      "source": [
        "searching two columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJdB2SNftXoR"
      },
      "source": [
        "# searching for conditional matches in two columns\n",
        "\n",
        "len(df[(df['Purchase Price'] > 95) & (df['CC Provider'] == 'American Express')])\n",
        "# or\n",
        "df[(df['Purchase Price'] > 95) & (df['CC Provider'] == 'American Express')].info()\n",
        "# or\n",
        "len(df[(df['Purchase Price'] > 95) & (df['CC Provider'] == 'American Express')].index)\n",
        "# or \n",
        "df[(df['Purchase Price'] > 95) & (df['CC Provider'] == 'American Express')].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUxXBELn051k"
      },
      "source": [
        "# split a string colum on a character and search just before or after split \n",
        "\n",
        "# new data frame with split value columns \n",
        "df1 = df[\"CC Exp Date\"].str.split(\"/\", n = 1, expand = True) \n",
        "# search conditional\n",
        "len(df1[df1[1]=='25'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnIhf13A8HS0"
      },
      "source": [
        "# split a string colum on a character and search just before or after split\n",
        "df[\"Email\"].apply(lambda email: email.split('@')[1]).value_counts().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI5-fy3y5gB4"
      },
      "source": [
        "# searching conditional for characters just after the 3rd place to the right\n",
        "df['CC Exp Date'].apply(lambda exp: exp[3:]=='25').value_counts()\n",
        "\n",
        "# or\n",
        "sum(df['CC Exp Date'].apply(lambda exp: exp[3:]=='25'))\n",
        "\n",
        "# or\n",
        "df[df['CC Exp Date'].apply(lambda exp: exp[3:]=='25')].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9bg6NuG6NDp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxLcKCbN6NAA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLkj2Skrk0DL"
      },
      "source": [
        "#Unique\n",
        "## Finding unique values, items, characters, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtzsAhs5k8gd"
      },
      "source": [
        "#unique values in column \n",
        "df['column_name'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajs_a9Fn57rf"
      },
      "source": [
        "# unique titles \n",
        "df['emp_title'].value_counts(dropna=False).reset_index().shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQq9OsDJbSwW"
      },
      "source": [
        "product_orders['product_name'].value_counts().sort_values(ascending=False)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFrMulpR-NnP"
      },
      "source": [
        "managers['int_rate'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaK7YTfn-rWN"
      },
      "source": [
        "df['issue_d'].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aueG34ykABHz"
      },
      "source": [
        "#returns array\n",
        "df['issue_d'].head().values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdHzEc2fAGPn"
      },
      "source": [
        "#checking to see if combinations appear multiple times.\n",
        "order_products.duplicated(subset=['order_id', 'product_id']).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR2ThZCyYnWv"
      },
      "source": [
        "order_products.duplicated(subset=['order_id', 'product_id']).value_counts().iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ua8ZLUoZwVG"
      },
      "source": [
        "order_products.duplicated(subset=['order_id', 'product_id']).value_counts().iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW75X_Ky9Fnn"
      },
      "source": [
        "##Missing Data (looking for where it is missing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMmvp-u09Edq"
      },
      "source": [
        "df.isnull().sum()\n",
        "df.isnull().sum(). sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgLluk9mzQav"
      },
      "source": [
        "##Basic Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHywBPbQ1CAz"
      },
      "source": [
        "# convert type to type\n",
        "# convert section\n",
        "# # this converts the datatype to integer\n",
        "# #df['dates'] = df['dates'].apply(lambda x: str(x))\n",
        "# df_time_analytics['month'] = df_time_analytics['month'].apply(lambda x: int(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veZmyfA2AtHv"
      },
      "source": [
        "# dropna fillna\n",
        "# and filling with mean or median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj2CcmsFAtXF"
      },
      "source": [
        "# by default, drops rows\n",
        "# same as\n",
        "#df.dropna(axis=0)\n",
        "df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BikbpqjHAtdB"
      },
      "source": [
        "# this changes it it drop collumns\n",
        "# but it can drop rows if you change the axis\n",
        "df.dropna(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBkq-PPsAtUE"
      },
      "source": [
        "# thresh: keeps rows with at least X number of non-NA (actual) values\n",
        "df.dropna(thresh=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AjRzfwaCA95"
      },
      "source": [
        "# fillna\n",
        "\n",
        "# parameter: value: fills NaN with that value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TNDNEeOCA61"
      },
      "source": [
        "# parameter: .mean()\n",
        "# this fills the NaN with mean values\n",
        "\n",
        "df['A'].fillna(value=df['A'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqvhWyBnCA4A"
      },
      "source": [
        "df['A'].fillna(value=df['A'].median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL7ELhSRAtRT"
      },
      "source": [
        "https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/learn/lecture/5733204#overview"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84WAMXkuZRdF"
      },
      "source": [
        "# replace NaN with mean values for collumn\n",
        "# https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns\n",
        "df.apply(lambda x: x.fillna(x.mean()),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aV-GaMasa5c"
      },
      "source": [
        "1.2) handle atypical missing value indicators and replace them with NaN values, (typical are marked as Nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nLNPEJnwtn0"
      },
      "source": [
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
        "\n",
        "https://colab.research.google.com/drive/1xqeB-Jk7LlsTV5KkHPwr0Jqpw6QHfMtd#scrollTo=OR80SCUoOl3i\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoXxP8SG4jDy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfIN35q44QXP"
      },
      "source": [
        "#look at the data at the source: look at individual cells if need be\n",
        "#df.iloc[14][13]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsA8MSaozotZ"
      },
      "source": [
        "# First, remove the comma separator, then cast to float\n",
        "df[\"LAND_SQUARE_FEET\"] = df[\"LAND_SQUARE_FEET\"].str.replace(\",\", \"\").astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cGc-D2N40-V"
      },
      "source": [
        "### get rid of floats amid strings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc3lH1CS41Na"
      },
      "source": [
        "#replaces (float)\"NaN\" with (str)\"unknown\"\n",
        "def clean_title(title):\n",
        "  if isinstance(title, str):\n",
        "    return title.strip().title()\n",
        "  else:\n",
        "    return \"Unknown\"\n",
        "\n",
        "df['emp_title'] = df['emp_title'].apply(clean_title)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pByYZd2_4eUO"
      },
      "source": [
        "\n",
        "###Repair Header / Footer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5-GENPB4ejd"
      },
      "source": [
        "#there is a problem, so the dataframe has to be reset:\n",
        "column_headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
        "                 'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
        "                 'capital-gain', 'capital-loss', 'hours-per-week', \n",
        "                 'native-country', 'income']\n",
        "\n",
        "df_adult = pd.read_csv('adult.data', names=column_headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE5uMBaZ6sxP"
      },
      "source": [
        "#this will skip the header and footer from the files being imported\n",
        "df = pd.read_csv('LoanStats_2018Q4.csv', header=1, na_values=['n/a'], skipfooter=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li258uh3zQko"
      },
      "source": [
        "\n",
        "#removes\n",
        "skipfooter=2\n",
        " \n",
        "# look for a footer to take out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy0EhxkXw3-8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1eRw9b5w4HV"
      },
      "source": [
        "# forward fill NaN\n",
        "df = df.fillna(method='ffill')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2L6vSvWcqad"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bERSDcpEw4ND"
      },
      "source": [
        "#backfill NaN\n",
        "df = df.fillna(method='backfill')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrWP3NRjwps-"
      },
      "source": [
        "#check again\n",
        "#checks for and summarizes missing data\n",
        "print(df.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ETP7rK-7HwV"
      },
      "source": [
        "#drop rows with NaN\n",
        "df1.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOYpmCCxWNMd"
      },
      "source": [
        "#Dropping columns\n",
        "\n",
        "http://www.datasciencemadesimple.com/drop-delete-rows-conditions-python-pandas/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0F1Zn31ZloI"
      },
      "source": [
        "# Drop a row by condition\n",
        "df = df[df.column_name != 'Fred']\n",
        "df = df[df.column_name == 'George']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpzwtP4DxEhF"
      },
      "source": [
        "## Correcting Missing and Strange Values in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf36_bp6xDuU"
      },
      "source": [
        "# Read in dataset with correct null values (as NaN)\n",
        "column_headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
        "                 'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
        "                 'capital-gain', 'capital-loss', 'hours-per-week', \n",
        "                 'native-country', 'income']\n",
        "\n",
        "# Example of replacing multiple null values\n",
        "df = pd.read_csv('adult.data', names=column_headers, na_values=[' ?', '999999', 'Unknown'])\n",
        "print(df.shape)\n",
        "df.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwqXm7iqKmbe"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAATq6kq-SbM"
      },
      "source": [
        "# test validate train split section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SEDOZO_-S29"
      },
      "source": [
        "# Split train into train & val. Make val the same size as test.\n",
        "target = 'status_group'\n",
        "train, val = train_test_split(train, test_size=len(test),  \n",
        "                              stratify=train[target], random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgMBhVgZKnic"
      },
      "source": [
        "#Wrangle Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Cjxzrwn_iL"
      },
      "source": [
        "%matplotlib inline\n",
        "import category_encoders as ce\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangles train, validate, and test sets in the same way\"\"\"\n",
        "    X = X.copy()\n",
        "\n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "    \n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']    \n",
        "    \n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "    \n",
        "    X = X.dropna()\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    duplicate_columns = ['quantity_group']\n",
        "    X = X.drop(columns=duplicate_columns)\n",
        "\n",
        "    # Drop empty NaN columns\n",
        "    empty_columns = ['quantity_group',\n",
        "                     'debt_settlement_flag_date',\n",
        "                     'settlement_status',\n",
        "\n",
        "url\n",
        "id\n",
        "member_id\n",
        "settlement_date\n",
        "settlement_amount\n",
        "settlement_percentage\n",
        "settlement_term  \n",
        "                     ]\n",
        "    X = X.drop(columns=empty_columns)\n",
        "    \n",
        "\n",
        "\n",
        "                         \n",
        "\n",
        "\n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these like null values\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, np.nan)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values\n",
        "    cols_with_zeros = ['construction_year', 'longitude', 'latitude', 'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        \n",
        "    return X\n",
        "\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
        "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val. Make val the same size as test.\n",
        "target = 'status_group'\n",
        "train, val = train_test_split(train, test_size=len(test),  \n",
        "                              stratify=train[target], random_state=42)\n",
        "\n",
        "# Wrangle train, validate, and test sets in the same way\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)\n",
        "\n",
        "# Arrange data into X features matrix and y target vector\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "#X_test = test #kaggle test set issues\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-OOtTMzqkhM"
      },
      "source": [
        "#### Define a function to wrangle train, validate, and test sets in the same way.\n",
        "\n",
        "Fix the location, and do more data cleaning and feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwOx78FgHxHp"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    cols_with_zeros = ['longitude', 'latitude']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "            \n",
        "    # quantity & quantity_group are duplicates, so drop one\n",
        "    X = X.drop(columns='quantity_group')\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y15wvTZ0ERgQ"
      },
      "source": [
        "# wrangle example 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0QeZ-72EXnj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-uPBl7XoAgA"
      },
      "source": [
        "#Wrangle example 3\n",
        "\n",
        "def wrangle(X):\n",
        "    X = X.copy()\n",
        "\n",
        "    # Engineer new feature for every feature: is the feature null?\n",
        "    for col in X:\n",
        "        X[col+'_NULL'] = X[col].isnull()\n",
        "    \n",
        "    # Convert percentages from strings to floats\n",
        "    X['int_rate'] = X['int_rate'].str.strip('%').astype(float)\n",
        "    X['revol_util'] = X['revol_util'].str.strip('%').astype(float)\n",
        "    \n",
        "    # Convert employment length from string to float\n",
        "    X['emp_length'] = X['emp_length'].str.replace(r'\\D','').astype(float)\n",
        "        \n",
        "    # Create features for three employee titles: teacher, manager, owner\n",
        "    X['emp_title'] = X['emp_title'].str.lower()\n",
        "    X['emp_title_teacher'] = X['emp_title'].str.contains('teacher', na=False)\n",
        "    X['emp_title_manager'] = X['emp_title'].str.contains('manager', na=False)\n",
        "    X['emp_title_owner']   = X['emp_title'].str.contains('owner', na=False)\n",
        "\n",
        "    # Get length of free text fields\n",
        "    X['title'] = X['title'].str.len()\n",
        "    X['desc'] = X['desc'].str.len()\n",
        "    X['emp_title'] = X['emp_title'].str.len()\n",
        "    \n",
        "    # Convert sub_grade from string \"A1\"-\"D5\" to numbers\n",
        "    sub_grade_ranks = {'A1': 1.1, 'A2': 1.2, 'A3': 1.3, 'A4': 1.4, 'A5': 1.5, \n",
        "                       'B1': 2.1, 'B2': 2.2, 'B3': 2.3, 'B4': 2.4, 'B5': 2.5, \n",
        "                       'C1': 3.1, 'C2': 3.2, 'C3': 3.3, 'C4': 3.4, 'C5': 3.5, \n",
        "                       'D1': 4.1, 'D2': 4.2, 'D3': 4.3, 'D4': 4.4, 'D5': 4.5}\n",
        "    X['sub_grade'] = X['sub_grade'].map(sub_grade_ranks)\n",
        "    \n",
        "    # Drop some columns\n",
        "    X = X.drop(columns='id')        # Always unique\n",
        "    X = X.drop(columns='url')       # Always unique\n",
        "    X = X.drop(columns='member_id') # Always null\n",
        "    X = X.drop(columns='grade')     # Duplicative of sub_grade\n",
        "    X = X.drop(columns='zip_code')  # High cardinality\n",
        "    \n",
        "    # Only use these features which had nonzero permutation importances in earlier models    \n",
        "    features = ['acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', \n",
        "                'annual_inc_joint', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \n",
        "                'collections_12_mths_ex_med', 'delinq_amnt', 'desc_NULL', 'dti', \n",
        "                'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_length_NULL', \n",
        "                'emp_title', 'emp_title_NULL', 'emp_title_owner', 'fico_range_high', \n",
        "                'funded_amnt', 'home_ownership', 'inq_last_12m', 'inq_last_6mths', \n",
        "                'installment', 'int_rate', 'issue_d_month', 'issue_d_year', 'loan_amnt', \n",
        "                'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', \n",
        "                'mo_sin_rcnt_rev_tl_op', 'mort_acc', 'mths_since_last_major_derog_NULL', \n",
        "                'mths_since_last_record', 'mths_since_recent_bc', 'mths_since_recent_inq', \n",
        "                'num_actv_bc_tl', 'num_actv_rev_tl', 'num_op_rev_tl', 'num_rev_tl_bal_gt_0', \n",
        "                'num_tl_120dpd_2m_NULL', 'open_rv_12m_NULL', 'open_rv_24m', \n",
        "                'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'purpose', \n",
        "                'revol_bal', 'revol_bal_joint', 'sec_app_earliest_cr_line', \n",
        "                'sec_app_fico_range_high', 'sec_app_open_acc', 'sec_app_open_act_il', \n",
        "                'sub_grade', 'term', 'title', 'title_NULL', 'tot_coll_amt', \n",
        "                'tot_hi_cred_lim', 'total_acc', 'total_bal_il', 'total_bc_limit', \n",
        "                'total_cu_tl', 'total_rev_hi_lim']    \n",
        "    X = X[features]\n",
        "    \n",
        "    # Reset index\n",
        "    X = X.reset_index(drop=True)\n",
        "    \n",
        "    # Return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "X_train = wrangle(X_train)\n",
        "X_val   = wrangle(X_val)\n",
        "X_test  = wrangle(X_test)\n",
        "\n",
        "print('X_train shape', X_train.shape)\n",
        "print('X_val shape', X_val.shape)\n",
        "print('X_test shape', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUPnFPug7l-S"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ8Y9YThouSv"
      },
      "source": [
        "# regex\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz7Eycc77mII"
      },
      "source": [
        "#Feature Scaling Notes\n",
        "#python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3frOw5zzbSP"
      },
      "source": [
        "##Encoding (and Dummy variables?)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQmZkMfqpaVQ"
      },
      "source": [
        "#regex\n",
        "import re\n",
        "re.sub(r'[^a-zA-Z ^0-9]', '', full_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdw31rOcRf1d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3V4ucCT8NBA"
      },
      "source": [
        "mask = {'set': 1, 'test': 2}\n",
        "\n",
        "df.replace({'set': mask, 'tesst': mask})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EbR7w5yt6GG"
      },
      "source": [
        "# quick way to get one-hot-encoding\n",
        "pd.get_dummies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ame0RkgzVSI"
      },
      "source": [
        "https://docs.google.com/document/d/15hF87e25XniWek9ueiE-AuTLWUcmO0csTprlJXLOXeQ/edit?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z4cDqyZzVcH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo1QTX-F4UUZ"
      },
      "source": [
        "# boolean dummy or numerical binary dummy\n",
        "#\n",
        "# create a white or not collum\n",
        "#sort of like a dummy variable, \n",
        "#this creates a new column based on creating values\n",
        "#which turn strings into a boolean (X or Not_X)\n",
        "#df['emp_title_manager'] = df['emp_title'].str.contains('Manager')\n",
        "df['white_or_not'] = df['race'].str.contains('White')\n",
        "\n",
        "#changes numerical column from the boolean\n",
        "df['white_or_not'].replace(True, 1, inplace=True)\n",
        "df['white_or_not'].replace(False, 0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQuyi-wjaBls"
      },
      "source": [
        "# replace all values in DF\n",
        "df.replace(['very bad', 'bad', 'poor', 'good', 'very good'], \n",
        "                     [1, 2, 3, 4, 5]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v696mpwPy3TG"
      },
      "source": [
        "#sort of like a dummy variable, \n",
        "#this creates a new column based on creating values\n",
        "#which turn strings into a boolean (X or Not_X)\n",
        "df['emp_title_manager'] = df['emp_title'].str.contains('Manager')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reO12bbLneHC"
      },
      "source": [
        "#https://www.geeksforgeeks.org/replacing-strings-with-numbers-in-python-for-data-analysis/\n",
        "# creating a dict file  \n",
        "gender = {'male': 1,'female': 2} \n",
        "  \n",
        "# traversing through dataframe \n",
        "# Gender column and writing \n",
        "# values where key matches \n",
        "df.Gender = [gender[item] for item in data.Gender] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-F270TtnvWB"
      },
      "source": [
        "#https://www.geeksforgeeks.org/replacing-strings-with-numbers-in-python-for-data-analysis/\n",
        "# creating a dict file  \n",
        "race = {'White': 1,'Black': 2} \n",
        "  \n",
        "# traversing through dataframe \n",
        "# Gender column and writing \n",
        "# values where key matches \n",
        "df.Gender = [race[item] for item in df.race] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD7mnmDzzViy"
      },
      "source": [
        "\n",
        "#Cleaning 1\n",
        "#This replaces True and False with 1, 0\n",
        "def replace_boolean(data):\n",
        "    for col in data:\n",
        "        data[col].replace(True, 1, inplace=True)\n",
        "        data[col].replace(False, 0, inplace=True)\n",
        "replace_boolean(test)\n",
        "replace_boolean(train)\n",
        "\n",
        "#Encoding catagorical strings as integers\n",
        "#sex\n",
        "Y_ranks = {\"functional needs repair\":2,\"functional\":1,\"non functional\":0}\n",
        "train_labels['status_group'] = train_labels.status_group.map(Y_ranks)\n",
        "\n",
        "#replacing with int replace\n",
        "#Encoding catagorical strings as integers\n",
        "#sex\n",
        "sex_ranks = {\"female\":0,\"male\":1}\n",
        "train.sex = train.sex.map(sex_ranks)\n",
        "test.sex = test.sex.map(sex_ranks)\n",
        "\n",
        "#embarked\n",
        "embarked_ranks = {\"C\":0,\"S\":1,\"Q\":2}\n",
        "train.embarked = train.embarked.map(embarked_ranks)\n",
        "test.embarked = test.embarked.map(embarked_ranks)\n",
        "\n",
        "#class1\n",
        "class1_ranks = {\"First\":0,\"Second\":1,\"Third\":2}\n",
        "train.class1 = train.class1.map(class1_ranks)\n",
        "test.class1 = test.class1.map(class1_ranks)\n",
        "\n",
        "#who\n",
        "who_ranks = {\"woman\":0,\"child\":1,\"man\":2}\n",
        "train.who = train.who.map(who_ranks)\n",
        "test.who = test.who.map(who_ranks)\n",
        "\n",
        "#replaced with binary replacement above: False True for 1,0\n",
        "#adult_male : \n",
        "#adultmale_ranks = {\"False\":0,\"True\":1}\n",
        "#train.adultmale = train.adultmale.map(adultmale_ranks)\n",
        "#test.adultmale = test.adultmale.map(adultmale_ranks)\n",
        "\n",
        "#deck: \n",
        "deck_ranks = {\"A\":0,\"B\":1,\"C\":2,\"D\":3,\"E\":4,\"F\":5}\n",
        "train.deck = train.deck.map(deck_ranks)\n",
        "test.deck = test.deck.map(deck_ranks)\n",
        "\n",
        "#embarktown: \n",
        "embarktown_ranks = {\"Southampton\":0,\"Queenstown\":1,\"Cherbourg\":2}\n",
        "train.embarktown = train.embarktown.map(embarktown_ranks)\n",
        "test.embarktown = test.embarktown.map(embarktown_ranks)\n",
        "\n",
        "#replaced with binary replacement above: False True for 1,0\n",
        "#alone: \n",
        "#alone_ranks = {\"False\":0,\"True\":1}\n",
        "#train.alone = train.alone.map(alone_ranks)\n",
        "#test.alone = test.alone.map(alone_ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSg23T3KCNcT"
      },
      "source": [
        "#Other-ing\n",
        "##Replace more then N diveristy/cardinality collumn items with \"other\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hperwcBMCMxb"
      },
      "source": [
        "# Reduce cardinality for NEIGHBORHOOD feature ...\n",
        " \n",
        "# Get a list of the top 10 neighborhoods\n",
        "top10 = train['NEIGHBORHOOD'].value_counts()[:10].index\n",
        "print(top10)\n",
        "\n",
        "# At locations where the neighborhood is NOT in the top 10,\n",
        "# replace the neighborhood with 'OTHER'\n",
        "train.loc[~train['NEIGHBORHOOD'].isin(top10), 'NEIGHBORHOOD'] = 'OTHER'\n",
        "test.loc[~test['NEIGHBORHOOD'].isin(top10), 'NEIGHBORHOOD'] = 'OTHER'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygUzNybaVBqB"
      },
      "source": [
        "# One Hot Encoding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tXVFmZ3HYIn"
      },
      "source": [
        "Use [OneHotEncoder](https://contrib.scikit-learn.org/categorical-encoding/onehot.html) from the [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding) library to encode any non-numeric features. (In this case, it's just `interest_level`.)\n",
        "\n",
        "- Use the **`fit_transform`** method on the **train** set\n",
        "- Use the **`transform`** method on **validation / test** sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZkJuIXTHYIo"
      },
      "source": [
        "import category_encoders as ce\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "X_train = encoder.fit_transform(X_train)\n",
        "X_test = encoder.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F44my_PZ1dHG"
      },
      "source": [
        "# export json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x2QTYIQ1f-Q"
      },
      "source": [
        "# this creates a json file...\n",
        "# setting a variable to the name of\n",
        "# the eventual desired json file\n",
        "your_file_name = existing_file\n",
        "#creates json file\n",
        "with open('your_file_name.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(your_file_name, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R1u70imGxwi"
      },
      "source": [
        "# index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df1LpFXXGyDy"
      },
      "source": [
        "# this shows the index\n",
        "df_indicator_search.index[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R75WXnMf6LYA"
      },
      "source": [
        "# Formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QkQgECC6Kny"
      },
      "source": [
        "    # for make jave compatible dictionaries, the data must be reformated according to these rules:\n",
        "    # Variable names cannot contain spaces.\n",
        "    # Variable names must begin with a letter, an underscore (_) or a dollar sign ($).\n",
        "    # Variable names can only contain letters, numbers, underscores, or dollar signs.\n",
        "    # Variable names are case-sensitive.\n",
        "    # \n",
        "    # https://stackoverflow.com/questions/1276764/stripping-everything-but-alphanumeric-chars-from-a-string-in-python#1277047\n",
        "\n",
        "    # function to reformat strings to be java dictionary keys\n",
        "    # based on notes here, but custom function and application\n",
        "    # https://stackoverflow.com/questions/1276764/stripping-everything-but-alphanumeric-chars-from-a-string-in-python#1277047\n",
        "    def format_string_for_java_dictionary_key(string1):\n",
        "      string1 = string1.strip()\n",
        "      string1 = re.sub(r'[^\\w\\s]','',string1)\n",
        "      string1 = string1.replace(\" \", \"_\")\n",
        "      string1 = \"_\"+string1\n",
        "      return string1\n",
        "      # one string example\n",
        "      #format_string_for_java_dictionary_key(\"  string. With. Punctuation?\")\n",
        "      # one column example\n",
        "      #df3['last_name'] = df3['last_name'].apply(lambda string1: format_string_for_java_dictionary_key(string1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxOYlDyM_aKy"
      },
      "source": [
        "##Time and Date, Time and Date, Date Time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbEzt--NLs-7"
      },
      "source": [
        "# this converts a datetime object to a posix epoch unix timestamp column\n",
        "#https://stackoverflow.com/questions/15203623/convert-pandas-datetimeindex-to-unix-time\n",
        "df_date['date_time_posix_epoch'] = df_date['date_object']\n",
        "df_dates.index = pd.DatetimeIndex(df_dates['date_time_posix_epoch'])\n",
        "df_dates['date_time_posix_epoch']= df_dates.index.astype(np.int64)// 10**9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPaLRbn6Ck6Z"
      },
      "source": [
        "# get range in months and years between entered start and end dates\n",
        "\n",
        "#user input\n",
        "year_start = 2001\n",
        "year_end = 2004\n",
        "month_start = 1\n",
        "month_end = 12\n",
        "\n",
        "# import libraries\n",
        "from datetime import datetime\n",
        "\n",
        "# function to find the range in months between two datetime objects:\n",
        "def diff_month(d1, d2):\n",
        "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
        "#sample end\n",
        "#diff_month(datetime(2010,10,1), datetime(2010,9,1))\n",
        "\n",
        "year_range = year_end - year_start + 1\n",
        "#print(year_range)\n",
        "total_months = diff_month(datetime(year_end,month_end,1), datetime(year_start,month_start,1))\n",
        "total_months\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjkQvC1g8quc"
      },
      "source": [
        "# month iterator\n",
        "\n",
        "import datetime, calendar\n",
        "\n",
        "def increment_month(date):\n",
        "    # Go to first of this month, and add 32 days to get to the next month\n",
        "    next_month = date.replace(day=1) + datetime.timedelta(32)\n",
        "    # Get the day of month that corresponds\n",
        "    day = min(date.day, calendar.monthrange(next_month.year, next_month.month)[1])\n",
        "    return next_month.replace(day=day)\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "year1 = 2011\n",
        "month1 = 12\n",
        "day1 = 1\n",
        "    \n",
        "day_range = 2\n",
        "# set range to the range of dates in that week\n",
        "end = day_range+1\n",
        "# enter starting date\n",
        "date = datetime.datetime(year1,month1,day1,0,0,0)\n",
        "\n",
        "increment_month(date)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2-leOvQz1ll"
      },
      "source": [
        "# day iterator\n",
        "\n",
        "# input values\n",
        "year1 = 2011\n",
        "month1 = 1 \n",
        "day1 = 21\n",
        "day_range = 14\n",
        "#\n",
        "\n",
        "# This is code the incriments a starting date set with variables to future dates\n",
        "# This overcomes issues of month and year dates restarting 'day' count\n",
        "# new output values\n",
        "year_new = 0 \n",
        "month_new = 0\n",
        "day_new = 0\n",
        "# enter starting date\n",
        "date = datetime.datetime(year1,month1,day1,0,0,0)\n",
        "# incriment date\n",
        "date += datetime.timedelta(days=1)\n",
        "# make split-able\n",
        "date_str = str(date)\n",
        "\n",
        "# pull out values to set new date variables\n",
        "year_new = date_str[0:4]\n",
        "# convert string to int\n",
        "year_new = int(year_new)\n",
        "\n",
        "month_new = date_str[5:7]\n",
        "# convert string to int\n",
        "month_new = int(month_new)\n",
        "\n",
        "day_new = date_str[8:11]\n",
        "# convert string to int\n",
        "day_new = int(day_new)\n",
        "\n",
        "# test print\n",
        "print(year_new)\n",
        "print(month_new)\n",
        "print(day_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guwc1HaXx3ky"
      },
      "source": [
        "# input values\n",
        "year1 = 2011\n",
        "month1 = 1 \n",
        "day1 = 21\n",
        "day_range = 14\n",
        "#\n",
        "# new output values\n",
        "year_new = 0 \n",
        "month_new = 0\n",
        "day_new = 0\n",
        "\n",
        "date = datetime.datetime(year1,month1,day1,0,0,0)\n",
        "for i in range(5): \n",
        "    date += datetime.timedelta(days=1)\n",
        "    print(date)\n",
        "    #print(type(date))\n",
        "    date_str = str(date) \n",
        "    year_new = date_str[0:4]\n",
        "    print(year_new)\n",
        "    month_new = date_str[5:7]\n",
        "    print(month_new)\n",
        "    day_new = date_str[8:11]\n",
        "    print(day_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZaqyzfevF1T"
      },
      "source": [
        "# #https://stackoverflow.com/questions/3240458/how-to-increment-a-datetime-by-one-day#3240486\n",
        "import datetime\n",
        "year1 = 2011\n",
        "month1 = 1 \n",
        "day1 = 21\n",
        "day_range = 14\n",
        "\n",
        "date = datetime.datetime(year1,month1,day1,0,0,0)\n",
        "for i in range(5): \n",
        "    date += datetime.timedelta(days=1)\n",
        "    print(date) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ5g83JviDH1"
      },
      "source": [
        "# sample date formatting\n",
        "\n",
        "df_dates = df.copy()\n",
        "# # Replaces 'na' with NaN\n",
        "df_dates = df_dates.replace('na', np.nan)\n",
        "# # Convert date_recorded to datetime\n",
        "df_dates['date_object'] = pd.to_datetime(df_dates['date'], infer_datetime_format=True)\n",
        "# # Edf_datestract components from date_recorded, then drop the original column\n",
        "df_dates['year'] = df_dates['date_object'].dt.year\n",
        "df_dates['month'] = df_dates['date_object'].dt.month\n",
        "#df_dates['date_unix_timestamp'] = datetime.datetime(df_dates['year'],df_dates['month'],1,0,0,0).strftime('%s')\n",
        "#df_dates['day'] = df_dates['date_object'].dt.day\n",
        "#this must be after data change...\n",
        "df_dates = df_dates.replace(np.nan, \"Null\")\n",
        "\n",
        "# this converts a datetime object to a posix epoch unix timestamp column\n",
        "#https://stackoverflow.com/questions/15203623/convert-pandas-datetimeindex-to-unix-time\n",
        "df_dates['date_time_posix'] = df_dates['date_object']\n",
        "df_dates.index = pd.DatetimeIndex(df_dates['date_time_posix'])\n",
        "#df_dates['date_time_posix']= df_dates.index.astype(np.int64)// 10**9\n",
        "df_dates['date_time_posix']= df_dates.index.astype(int)// 10**9\n",
        "\n",
        "df_dates = df_dates.reset_index(drop=True)\n",
        "df = df_dates.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hITE8ohTIhQx"
      },
      "source": [
        "def start_of_week(year, month, day):\n",
        "    \n",
        "    import datetime\n",
        "    #today = datetime.date.today()\n",
        "    today = datetime.datetime(year, month, day)\n",
        "    #today = pd.to_datetime(df2['date'], infer_datetime_format=True)\n",
        "    #datetime.date(2013, 8, 13)\n",
        "    #print(today)\n",
        "    idx = (today.weekday() + 1) % 7 # MON = 0, SUN = 6 -> SUN = 0\n",
        "    #print(idx)\n",
        "    date_of_previous_sunday = today - datetime.timedelta(idx)\n",
        "    start_of_week = str(date_of_previous_sunday)[8:10]\n",
        "    #print(date_of_previous_sunday)\n",
        "    #start_of_week = str(date_of_previous_sunday)[-2:]\n",
        "    #start_of_week = int(start_of_week)\n",
        "    return start_of_week\n",
        "\n",
        "    start_of_week(2011,1,21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_AWOQWQIiVR"
      },
      "source": [
        "    df2 = df\n",
        "    # # Replaces 'na' with NaN\n",
        "    df2 = df2.replace('na', np.nan)\n",
        "    # # Convert date_recorded to datetime\n",
        "    df2['date_object'] = pd.to_datetime(df2['date'], infer_datetime_format=True)\n",
        "    # # Edf2tract components from date_recorded, then drop the original column\n",
        "    df2['year'] = df2['date_object'].dt.year\n",
        "    df2['week'] = df2['date_object'].dt.week\n",
        "    df2['month'] = df2['date_object'].dt.month\n",
        "    df2['day'] = df2['date_object'].dt.day\n",
        "\n",
        "    #this must be after data change...\n",
        "    df2 = df2.replace(np.nan, \"Null\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uWzff7IP2D2"
      },
      "source": [
        "\n",
        "    # making the new time-columns...from scratch as required by team...\n",
        "    df2 = df.copy()\n",
        "    # # Replaces 'na' with NaN\n",
        "    df2 = df2.replace('na', np.nan)\n",
        "\n",
        "\n",
        "    # # Convert date_recorded to datetime\n",
        "    df2['date_object'] = pd.to_datetime(df2['date'], infer_datetime_format=True)\n",
        "    # # Edf2tract components from date_recorded, then drop the original column\n",
        "    df2['year'] = df2['date_object'].dt.year\n",
        "    df2['month'] = df2['date_object'].dt.month\n",
        "    df2['day'] = df2['date_object'].dt.day\n",
        "    \n",
        "    #this must be after data change...\n",
        "    df2 = df2.replace(np.nan, \"Null\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W00yHdjr_cD4"
      },
      "source": [
        "# converts from\n",
        "# Dec-2018 form (or any form?)\n",
        "# to\n",
        "# 2018-12-01 \n",
        "# form\n",
        "\n",
        "#df['issue_d'] = pd.to_datetime(df['issue_d'], infer_datetime_format=True)\n",
        "df[\"created\"] = pd.to_datetime(df[\"created\"], infer_datetime_format=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knny49AL_cIL"
      },
      "source": [
        "#creates a new column by taking just year out of date\n",
        "\n",
        "df['year'] = df['issue_d'].dt.year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huu1lVFi_cLF"
      },
      "source": [
        "#creates a new column by taking just month out of date\n",
        "df['month'] = df['issue_d'].dt.month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xbC2u4c-M4t"
      },
      "source": [
        "#creates a new column by taking just month out of date\n",
        "df['day'] = df['issue_d'].dt.day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCkrgDC8Djhv"
      },
      "source": [
        "# converts formats \n",
        "# splits into separate column\n",
        "df[\"created\"] = pd.to_datetime(df[\"created\"], infer_datetime_format=True)\n",
        "df['year'] = df['issue_d'].dt.year\n",
        "df['month'] = df['issue_d'].dt.month\n",
        "df['day'] = df['issue_d'].dt.day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URzHMZeC-S1R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjw7xqbJ_cOA"
      },
      "source": [
        "#returns difference between two dates\n",
        "df['issue_d'] - df['earliest_cr_line']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMgocDF3Ilhv"
      },
      "source": [
        "# time based split\n",
        "# create a test train split \n",
        "# remember to put time in quotes\n",
        "df[\"created\"] = pd.to_datetime(df[\"created\"], infer_datetime_format=True)\n",
        "train = df[df['created'] < '2016-06-01 00:00:00']\n",
        "test = df[df['created'] >= '2016-06-01 00:00:00']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AypWcahtR5kG"
      },
      "source": [
        "#train test split with random seed\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "my_train, my_val = train_test_split(df, random_state=42)\n",
        "my_train.shape, my_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhmRRR13bPqi"
      },
      "source": [
        "#train test split with random seed\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
        "train.shape, test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUwZsiylQtci"
      },
      "source": [
        "#train validate split with random seed\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, validate = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train.shape, validate.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd20pjPCQvfl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzhMIwjXXCOd"
      },
      "source": [
        "#Feature Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv_DaBzSteVL"
      },
      "source": [
        "https://www.geeksforgeeks.org/python-pandas-split-strings-into-two-list-columns-using-str-split/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyLuxp60HYJC"
      },
      "source": [
        "### Could we try every possible feature combination?\n",
        "\n",
        "The number of [combinations](https://en.wikipedia.org/wiki/Combination) is shocking!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTFs0lruHYJD"
      },
      "source": [
        "# How many features do we have currently?\n",
        "features = X_train.columns\n",
        "n = len(features)\n",
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dQX64n5HYJG"
      },
      "source": [
        "# How many ways to choose 1 to n features?\n",
        "from math import factorial\n",
        "\n",
        "def n_choose_k(n, k):\n",
        "    return factorial(n)/(factorial(k)*factorial(n-k))\n",
        "\n",
        "combinations = sum(n_choose_k(n,k) for k in range(1,n+1))\n",
        "print(f'{combinations:,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr96i7bc_yTf"
      },
      "source": [
        "# change datatype of column\n",
        "# BOROUGH is a numeric column, but arguably should be a categorical feature,\n",
        "# so convert it from a number to a string\n",
        "df['BOROUGH'] = df['BOROUGH'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nShw73UkYSl2"
      },
      "source": [
        "##Select best features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pdPsGfRHYJM"
      },
      "source": [
        "Refer to the [Scikit-Learn User Guide on Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-MsLukDHYJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc66dcf4-f193-4b12-b5b3-fe04501e972e"
      },
      "source": [
        "# Select the 15 features that best correlate with the target\n",
        "# (15 is an arbitrary starting point here)\n",
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "\n",
        "# Similar API to what we've seen before\n",
        "selector = SelectKBest(score_func=f_regression, k=15)\n",
        "\n",
        "# IMPORTANT!\n",
        "# .fit_transform on the train set\n",
        "# .transform on test set\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "X_train_selected.shape, X_test_selected.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((31844, 15), (16973, 15))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B4nFysYHYJR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "5ca3a9ac-553d-42fb-b658-d5d12b48c2ad"
      },
      "source": [
        "# Which features were selected?\n",
        "all_names = X_train.columns\n",
        "selected_mask = selector.get_support()\n",
        "selected_names = all_names[selected_mask]\n",
        "unselected_names = all_names[~selected_mask]\n",
        "\n",
        "print('Features selected:')\n",
        "for name in selected_names:\n",
        "    print(name)\n",
        "    \n",
        "print('\\n')\n",
        "print('Features not selected:')\n",
        "for name in unselected_names:\n",
        "    print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features selected:\n",
            "bathrooms\n",
            "bedrooms\n",
            "longitude\n",
            "interest_level_high\n",
            "interest_level_low\n",
            "elevator\n",
            "doorman\n",
            "dishwasher\n",
            "fitness_center\n",
            "laundry_in_unit\n",
            "outdoor_space\n",
            "dining_room\n",
            "terrace\n",
            "perk_count\n",
            "rooms\n",
            "\n",
            "\n",
            "Features not selected:\n",
            "latitude\n",
            "interest_level_medium\n",
            "cats_allowed\n",
            "hardwood_floors\n",
            "dogs_allowed\n",
            "no_fee\n",
            "laundry_in_building\n",
            "pre-war\n",
            "roof_deck\n",
            "high_speed_internet\n",
            "balcony\n",
            "swimming_pool\n",
            "new_construction\n",
            "exclusive\n",
            "loft\n",
            "garden_patio\n",
            "wheelchair_access\n",
            "common_outdoor_space\n",
            "cats_or_dogs\n",
            "cats_and_dogs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eKEaRDwAXbP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABVH9OuyAXvK"
      },
      "source": [
        "#Feature Importance\n",
        "- feature permutation/\n",
        "Permutation Importance (by how much does your prediction score decrease if you remove that feature)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyatDHtVAb2E"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "preprocessor = ce.OrdinalEncoder()\n",
        "X_train_transformed = preprocessor.fit_transform(X_train[features])\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    class_weight='balanced',\n",
        "    n_job=-1)\n",
        "\n",
        "model.fit(X_train_transformed, y_train)\n",
        "permuter = PermutationImportance(model, \n",
        "                                 scoring='roc_auc', \n",
        "                                 n_iter=5, \n",
        "                                 cv='prefit')\n",
        "permuter.fit(X_train_transformed, y_train)\n",
        "eli5.show_weights(permuter, top=None, feature_names=features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_yCVNNVAcIT"
      },
      "source": [
        "##.Apply\n",
        "\n",
        "s.apply(func, convert_dtype=True, args=())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uS5NBIkAcjT"
      },
      "source": [
        "# defining function to check price \n",
        "def function_name(item): \n",
        "        return item+2\n",
        "  \n",
        "# passing function to apply and storing returned series in new \n",
        "new = s.apply(function) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii3X984mBYZt"
      },
      "source": [
        "# Write a function that will work on any given cell in a dataframe column\n",
        "def remove_percent_to_float(string):\n",
        "  return float(string.strip('%'))\n",
        "\n",
        "remove_percent_to_float(int_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H2w5a9-BhX-"
      },
      "source": [
        "apply function to column_name\n",
        "remembmer to return a value!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U87EqYqhFOen"
      },
      "source": [
        "train['construction_year'].replace({'a': {'b': np.nan}}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGs9fW0yBW1L"
      },
      "source": [
        "df['int_rate'] = df['int_rate'].apply(remove_percent_to_float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1LIxg4D8Yp3"
      },
      "source": [
        "##Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN6J1C1tRi-A"
      },
      "source": [
        "# if conditional\n",
        "# if the contents of x_symptom is true, then make a merged text string engineered feature\n",
        "df.loc[df['spasticity_symptom'] == True, 'symptoms_diseases'] = df[\"spasticity_text\"]+ \", \" + df[\"symptoms_diseases\"].map(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFOcQHh3FPHJ"
      },
      "source": [
        "# replacing a given item with NaN, replace with NaN\n",
        "train['construction_year'] = train['construction_year'].replace('0', np.NaN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwet0mqlHcgU"
      },
      "source": [
        "#or for the whole df:\n",
        "\n",
        "df.replace('?', np.NaN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7TlerH78YxW"
      },
      "source": [
        "#use filter to fill a new column with boolean yes if contains X\n",
        "df['emp_title_manager'] = df['emp_title'].str.contains('X')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8F6uAYyFDgg"
      },
      "source": [
        "#removing columns\n",
        "df.drop(index='cow', columns='small')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgHXklJk7s30"
      },
      "source": [
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
        "\n",
        "DataFrame.drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys5ko-wqnvmv"
      },
      "source": [
        "# fill column with one value\n",
        "df['A'] = 'foo'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OQDU1g8blN"
      },
      "source": [
        "# This pulls a number from the front of a string and turns that into a catagorical\n",
        "df['feature'] = df['feature'].str.split('.').str[0]\n",
        "# this show what that produced\n",
        "df['feature'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Vk-yGO6DGW"
      },
      "source": [
        "\n",
        "# change decimals change the number of decimal places\n",
        "arr3 = np.around(arr3, decimals=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiUhf4n8nNJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YCD_oH67orG"
      },
      "source": [
        "\n",
        "##linspace vs. stepsize arange\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EImD2V2o7mX1"
      },
      "source": [
        "# step size (parameters: starting value, up-to value, stepsize)\n",
        ">>> np.arange(0,11,2)\n",
        "-> array([ 0,  2,  4,  6,  8, 10])\n",
        "\n",
        "# vs\n",
        "#evenly spaced points within range as opposed to stepsize (fixed gap)\n",
        "# parameters: (start, stop, number of intermediate numbers)\n",
        "\n",
        ">>> np.linspace(0,5,10) \n",
        "-> array([0.        , 0.55555556, 1.11111111, 1.66666667, 2.22222222,\n",
        "       2.77777778, 3.33333333, 3.88888889, 4.44444444, 5.        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UMH-BfFuyko"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIZ25Aey6vDg"
      },
      "source": [
        "\"click tab\" to view methods...juypter notebook?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpPTZfybvRWr"
      },
      "source": [
        "## Feature Form Translation\n",
        "e.g. for list functions that don't work well on columns\n",
        "\n",
        "map(apply function to all elements)\n",
        "\n",
        "filter(leaving only boolean True)\n",
        "\n",
        "split(on this)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwZjouvsu0Si"
      },
      "source": [
        "\n",
        "\n",
        "### dataframe/series to list\n",
        "\n",
        "https://stackoverflow.com/questions/22341271/get-list-from-pandas-dataframe-column\n",
        "\n",
        "Pandas DataFrame columns are Pandas Series when you pull them out, which you can then call x.tolist() on to turn them into a Python list. Alternatively you cast it with list(x).\n",
        "\n",
        "dfToList = df['one'].tolist()\n",
        "\n",
        "dfList = list(df['one'])\n",
        "\n",
        "dfValues = df['one'].values\n",
        "\n",
        "in => 'i' in list_name(or raw list)\n",
        "\n",
        "tuple unpacking\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCnPRTk8vajR"
      },
      "source": [
        "list to dataframe/series\n",
        "\n",
        "https://stackoverflow.com/questions/42049147/convert-list-to-pandas-dataframe-column\n",
        "\n",
        "df = pd.DataFrame({'new_column_name':list_name})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jeh0jdSEv1t"
      },
      "source": [
        "# matrix matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiy2z7os-dmm"
      },
      "source": [
        "# make a matrix in one line:\n",
        "# (parameters: (start, up to, stepsize) and (dimensions of matrix)\n",
        "mat = np.arange(1,26,1).reshape(5,5)\n",
        "mat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1zxyB24Eqzv"
      },
      "source": [
        "# RC rows before colums\n",
        "mat3 = np.array([2,7,12]).reshape(3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkqa5iP8Iffr"
      },
      "source": [
        "https://www.sharpsightlabs.com/blog/numpy-sum/\n",
        "# parameters\n",
        "np.sum(array or matrix, axis of matrix 0 or 1 etc, dtype of output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ils-4aL8KbL"
      },
      "source": [
        "##String slice\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6LW1Rfu80RT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmDaXV5O8MQ8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZzsdnbs8Nru"
      },
      "source": [
        "##Training Set, Test Set, Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUPT5KDpzWwg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2KCKOoEx_9L"
      },
      "source": [
        "##Cross Tables\n",
        "\n",
        "https://colab.research.google.com/drive/19eUlRf1m1NGYP6IddyikG3OKB-IhIvE7\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyuJmuUSyAVl"
      },
      "source": [
        " #generate crosstable\n",
        " pd.crosstab(df.regiment, df.company, margins=True)\n",
        "\n",
        "\n",
        " # with headings!\n",
        " #Let's use crosstabulation to try to see what's going on\n",
        "pd.crosstab(user_data['purchased'], user_data['time_on_site'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBIT219N_hfj"
      },
      "source": [
        "##Filtering section filt sec filter section\n",
        "\n",
        "https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/learn/lecture/5733214#overview\n",
        "\n",
        "https://chrisalbon.com/python/data_wrangling/filter_dataframes/\n",
        "\n",
        "https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/\n",
        "\n",
        "Note: \n",
        "Zip function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOAp6wLM_bo1"
      },
      "source": [
        "# insane boolean true value filtering\n",
        "# 1 make a conditional copy of a DF\n",
        "# \"compare\" the two dataframes by putting the 2nd dataframe into the first!\n",
        "# make a new DF containing only the filtered values\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import randn\n",
        "np.random.seed(101)\n",
        "df = pd.DataFrame(randn(5,4),['A','B','C','D','E'],['W','X','Y','Z'])\n",
        "df_boolean = df < 0\n",
        "df_filtered = df[df_boolean]\n",
        "df_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uduwJN3YXCcw"
      },
      "source": [
        "#filter out the features you want keep\n",
        "new_df = old_df.filter(['A','B','D'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Y2poSkkmeW"
      },
      "source": [
        "#get all the features for one observation\n",
        "X_test.iloc[[3094]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfxRbSzyi3n5"
      },
      "source": [
        "#reverses colum order of output\n",
        "()[::-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym71tEkPsHw-"
      },
      "source": [
        ".sort_values('Credit Score', ascending=False) #works as well if the slicing confuses you."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTQgzsSyyqJc"
      },
      "source": [
        "#so changes made to new don't affect old\n",
        "new_df = df.copy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ueHTqgSrcaP"
      },
      "source": [
        "#sort by values\n",
        "#lists by values\n",
        "pd.DataFrame(search.cv_results_).sort_values(by='rank_test_score')\n",
        "\n",
        "#adds transpose\n",
        "pd.DataFrame(search.cv_results_).sort_values(by='rank_test_score').T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDAiT5DOiXRt"
      },
      "source": [
        "#normalized value count!\n",
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBD0R7vlE7I"
      },
      "source": [
        "# as a way of filtering whole dataframe by collumn\n",
        "df = df[df['SALE_PRICE'] > 0]\n",
        "df = df[df['SALE_PRICE'] > 0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "582Lf4NkalWb"
      },
      "source": [
        "#use \"filtering\" to create two new party based df (so much for nonpartisan dataframes...a sad day)\n",
        "dem = df[df['party'] == 'democrat']\n",
        "rep = df[df['party'] == 'republican']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1b20L5ZBmcX"
      },
      "source": [
        "#value_counts: prints out results in ascending or descending order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qtDVuS__gzy"
      },
      "source": [
        "#top ten products\n",
        "orders_products['product_name'].value_counts().head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76jfFbZX_g26"
      },
      "source": [
        "#doing computations or functions on each item in collum:\n",
        "#.apply\n",
        "\n",
        "def starts_with_m(item):\n",
        "   return string[0]=='m'\n",
        "\n",
        "df['sex'].apply(starts_with_m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXqzfahA0PaH"
      },
      "source": [
        "# get the number of unique employment titles\n",
        "df['emp_title'].value_counts(dropna=False).reset_index().shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z83P6Fmu1XHf"
      },
      "source": [
        "# empty slots\n",
        "df['emp_title'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVD3CImv7V-4"
      },
      "source": [
        "#returns boolean if contains x\n",
        "df['emp_title'].str.contains('Manager')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DScUP-v-7WGR"
      },
      "source": [
        "#returns the whole df but only those parts that contain the boolean yes\n",
        "new_variable = (df['emp_title_manager'] == True)\n",
        "df[new_variable].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL-NeWWW8qpM"
      },
      "source": [
        "#returns part of df that contaions a given string XXX\n",
        "new_variable = df[df['emp_title'].str.contains('XXX')]\n",
        "print(new_variable.shape)\n",
        "new_variable.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKnLHSYc85A1"
      },
      "source": [
        "#returns the whole df but only those parts that contain the boolean yes\n",
        "top10_orders_and_products = orders_products[orders_products['keep_it'] == True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6FV2mKb85DW"
      },
      "source": [
        "#recomended method to filter...\"dataframe filtering vs .loc\"\n",
        "usa = df[df.country == 'United States']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPqO5slebNwv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt10V40VSwSL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHTOi2mFwqAK"
      },
      "source": [
        "# Baseline Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4KTkvAN7ORx"
      },
      "source": [
        "Notes on baseline:\n",
        "\n",
        "examples of baseline:\n",
        "1. Mean of Y\n",
        "2. (r)SSE of mean of Y\n",
        "3. Kitchen sink model (all features) (2018 teacher)\n",
        "4. random guess (w/cross val) (R.Herr 2019)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtc2836Swrkq"
      },
      "source": [
        "# This returns the decimal/percent ratios of the component \"classes\"  cardinals, categories, options\n",
        "df.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2EGeBEIzx2H"
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.ensamble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "#This is a pipeline to calculate a baseline based on just one feature\n",
        "#Note: this contains a cross-validation element, so 3-way-split is not needed.\n",
        "features = ['one_X_feature']\n",
        "\n",
        "baseline_pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        class_weight='balanced',\n",
        "        n_jobs=-1,\n",
        "    )\n",
        ")\n",
        "\n",
        "cross_val_score(baseline_pipeline, \n",
        "                X_train[features], \n",
        "                y_train, \n",
        "                cv=5,\n",
        "                scoring='roc_auc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa8JJA8Ky7F2"
      },
      "source": [
        "#Graphs plotting \n",
        "graphing section viz fig graph chart plot dashboard visualziation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq5e-CgUZ00x"
      },
      "source": [
        "#basic plot\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.distplot(df['cost']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pLL1HPeyC6a"
      },
      "source": [
        "#basic plot\n",
        "df['column_name'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSJlPNNC8goH"
      },
      "source": [
        "#Graphs\n",
        "\n",
        " \n",
        "#Histogram \n",
        "plotA = df['age'].hist(bins=100);\n",
        "plotA.set_ylabel('People Observed')\n",
        "plotA.set_xlabel('Age')\n",
        "plotA.set_title('Age By number of People')\n",
        "\n",
        "\n",
        "#Scatterplot\n",
        "plotae = df.plot.scatter('age', 'exercise_time');\n",
        "plotae.set_ylabel('exercise_timeht')\n",
        "plotae.set_xlabel('Age')\n",
        "plotae.set_title('Age and exercise_time')\n",
        "\n",
        "\n",
        "#Density Plot\n",
        "plotdw = df['weight'].plot.density();\n",
        "plotdw.set_xlabel('Weight')\n",
        "plotdw.set_title('density plot of Weight')\n",
        "dataset1['ethnicity'].plot.density();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP10hIr19fK6"
      },
      "source": [
        "Graph a single column..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kh7Xd7x9iUG"
      },
      "source": [
        "#density plot\n",
        "df['int_rate'].plot.density()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tijm6Blm9iY9"
      },
      "source": [
        "#hist\n",
        "df['int_rate'].hist(bins=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M42b2EBLPce"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPGdxIBDLUT2"
      },
      "source": [
        "Matplot Lib Basics from udemy course\n",
        "\n",
        "https://colab.research.google.com/drive/1jz0D8OnSgZUhTg4xwapLVAKikuMyWQFH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j53uQL5k9fUw"
      },
      "source": [
        "## Template for Matplotlib\n",
        "\n",
        "https://colab.research.google.com/drive/1yv6VW-gwXoJ_BjatTqG_XRrHCSQ5F310#scrollTo=5EqXxnJeB89_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftKZwVEyLMx5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvPJnqS1VGcl"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import AutoMinorLocator, MultipleLocator, FuncFormatter\n",
        "\n",
        "np.random.seed(19680801)\n",
        "\n",
        "#generating data to put on figure\n",
        "X = np.linspace(0.5, 3.5, 100)\n",
        "Y1 = 3+np.cos(X)\n",
        "Y2 = 1+np.cos(1+X/0.75)/2\n",
        "Y3 = np.random.uniform(Y1, Y2, len(X))\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1, aspect=1)\n",
        "\n",
        "def minor_tick(x, pos):\n",
        "    if not x % 1.0:\n",
        "        return \"\"\n",
        "    return \"%.2f\" % x\n",
        "\n",
        "#on axes object\n",
        "ax.xaxis.set_major_locator(MultipleLocator(1.000))\n",
        "ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
        "ax.yaxis.set_major_locator(MultipleLocator(1.000))\n",
        "ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
        "ax.xaxis.set_minor_formatter(FuncFormatter(minor_tick))\n",
        "\n",
        "ax.set_xlim(0, 4)\n",
        "ax.set_ylim(0, 4)\n",
        "\n",
        "ax.tick_params(which='major', width=1.0)\n",
        "ax.tick_params(which='major', length=10)\n",
        "ax.tick_params(which='minor', width=1.0, labelsize=10)\n",
        "ax.tick_params(which='minor', length=5, labelsize=10, labelcolor='0.25')\n",
        "\n",
        "ax.grid(linestyle=\"--\", linewidth=0.5, color='.25', zorder=-10)\n",
        "\n",
        "#ax.plot(X, Y1, c=(0.25, 0.25, 1.00), lw=2, label=\"Blue signal\", zorder=10)\n",
        "ax.plot(X, Y2, c=(1.00, 0.25, 0.25), lw=2, label=\"Red signal\")\n",
        "ax.plot(X, Y3, linewidth=0,\n",
        "        marker='o', markerfacecolor='w', markeredgecolor='k')\n",
        "\n",
        "ax.set_title(\"Anatomy of a figure\", fontsize=20, verticalalignment='bottom')\n",
        "ax.set_xlabel(\"X axis label\")\n",
        "ax.set_ylabel(\"Y axis label\")\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "def circle(x, y, radius=0.15):\n",
        "    from matplotlib.patches import Circle\n",
        "    from matplotlib.patheffects import withStroke\n",
        "    circle = Circle((x, y), radius, clip_on=False, zorder=10, linewidth=1,\n",
        "                    edgecolor='black', facecolor=(0, 0, 0, .0125),\n",
        "                    path_effects=[withStroke(linewidth=5, foreground='w')])\n",
        "    ax.add_artist(circle)\n",
        "\n",
        "\n",
        "def text(x, y, text):\n",
        "    ax.text(x, y, text, backgroundcolor=\"white\",\n",
        "            ha='center', va='top', weight='bold', color='blue')\n",
        "\n",
        "# Minor tick\n",
        "circle(0.50, -0.10)\n",
        "text(0.50, -0.32, \"Minor tick label\")\n",
        "\n",
        "# Major tick\n",
        "circle(-0.03, 4.00)\n",
        "text(0.03, 3.80, \"Major tick\")\n",
        "\n",
        "# Minor tick\n",
        "circle(0.00, 3.50)\n",
        "text(0.00, 3.30, \"Minor tick\")\n",
        "\n",
        "# Major tick label\n",
        "circle(-0.15, 3.00)\n",
        "text(-0.15, 2.80, \"Major tick label\")\n",
        "\n",
        "# X Label\n",
        "circle(1.80, -0.27)\n",
        "text(1.80, -0.45, \"X axis label\")\n",
        "\n",
        "# Y Label\n",
        "circle(-0.27, 1.80)\n",
        "text(-0.27, 1.6, \"Y axis label\")\n",
        "\n",
        "# Title\n",
        "circle(1.60, 4.13)\n",
        "text(1.60, 3.93, \"Title\")\n",
        "\n",
        "# Blue plot\n",
        "circle(1.75, 2.80)\n",
        "text(1.75, 2.60, \"Line\\n(line plot)\")\n",
        "\n",
        "# Red plot\n",
        "circle(1.20, 0.60)\n",
        "text(1.20, 0.40, \"Line\\n(line plot)\")\n",
        "\n",
        "# Scatter plot\n",
        "circle(3.20, 1.75)\n",
        "text(3.20, 1.55, \"Markers\\n(scatter plot)\")\n",
        "\n",
        "# Grid\n",
        "circle(3.00, 3.00)\n",
        "text(3.00, 2.80, \"Grid\")\n",
        "\n",
        "# Legend\n",
        "circle(3.70, 3.80)\n",
        "text(3.70, 3.60, \"Legend\")\n",
        "\n",
        "# Axes\n",
        "circle(0.5, 0.5)\n",
        "text(0.5, 0.3, \"Axes\")\n",
        "\n",
        "# Figure\n",
        "circle(-0.3, 0.65)\n",
        "text(-0.3, 0.45, \"Figure\")\n",
        "\n",
        "color = 'blue'\n",
        "ax.annotate('Spines', xy=(4.0, 0.35), xytext=(3.3, 0.5),\n",
        "            weight='bold', color=color,\n",
        "            arrowprops=dict(arrowstyle='->',\n",
        "                            connectionstyle=\"arc3\",\n",
        "                            color=color))\n",
        "\n",
        "ax.annotate('', xy=(3.15, 0.0), xytext=(3.45, 0.45),\n",
        "            weight='bold', color=color,\n",
        "            arrowprops=dict(arrowstyle='->',\n",
        "                            connectionstyle=\"arc3\",\n",
        "                            color=color))\n",
        "\n",
        "ax.text(4.0, -0.4, \"Made with http://matplotlib.org\",\n",
        "        fontsize=10, ha=\"right\", color='.5')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1J5svmSzmKJ"
      },
      "source": [
        "##Quick Correlation Check\n",
        "\n",
        "corr section\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb6IGeGBzmT2"
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w6b5HyYCcHr"
      },
      "source": [
        "df[['Column1', 'Column2']].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66JhMO6OC6BW"
      },
      "source": [
        "import numpy as np\n",
        "X = df['Column1']\n",
        "y = df['Column2']\n",
        "np.corrcoef(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2487Y6mD0HZt"
      },
      "source": [
        "##Binning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bbqzjNO0Hh8"
      },
      "source": [
        "#titanic dataset\n",
        "#q cut in quantile sized bins\n",
        "# 5 is arbitary\n",
        "age_bins_cut = pd.cut(df['age'], 5)\n",
        "age_bins_qcut = pd.qcut(df['age'], 5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md2cW4bw1AbV"
      },
      "source": [
        "#equal size intervals\n",
        "age_bins_cut.value_counts(sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "845uf_161S1D"
      },
      "source": [
        "#equal size intervals\n",
        "age_bins_qcut.value_counts(sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mJQ8R0s18v0"
      },
      "source": [
        "control for:\n",
        "g = sns.FacetGrid(df, col=\"age_bin\", height=4)\n",
        "g.map(sns.regplot, \"exercise_time\", \"weight\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLx_SnY4QZFS"
      },
      "source": [
        "##Join Merge Contacotonate Melt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8QbGvGUAuWb"
      },
      "source": [
        "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsLS4XjXQc5O"
      },
      "source": [
        "# merges them if there is one shared collumn\n",
        "pd.concat([df1,df2], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V41UKdAAQc96"
      },
      "source": [
        "# stacks them if all collumns are already the same\n",
        "pd.concat([df1,df2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuXLihICQdAN"
      },
      "source": [
        "#melt: makes it more tidy\n",
        "pd.melt(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FolX4XyvQc8Q"
      },
      "source": [
        "#inner join in python pandas\n",
        "print pd.merge(df1, df2, on='Customer_id', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSOMqU1zOyNa"
      },
      "source": [
        "#drop all columns except a, b\n",
        "df.drop(df.columns.difference(['a','b']), 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh2epkZVasf7"
      },
      "source": [
        "#Turn an index into a column\n",
        "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNhQJ22X9gWU"
      },
      "source": [
        "# Statistics Section\n",
        "Stats\n",
        "stat sec\n",
        "stats sec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfYbF-I089Oc"
      },
      "source": [
        "https://colab.research.google.com/drive/1MX0LJlRgObHoJ2sC2xx5X-n3E7Wn_2p9#scrollTo=62rXAnuhM7xs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecCEJ6yD9sF7"
      },
      "source": [
        "t_df10 = np.random.standard_t(df=10, size=10)\n",
        "t_df100 = np.random.standard_t(df=100, size=1000)\n",
        "t_df1000 = np.random.standard_t(df=1000, size=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HW13wtD9rkx"
      },
      "source": [
        "sns.kdeplot(t_df10, color='r');\n",
        "sns.kdeplot(t_df100, color='y');\n",
        "sns.kdeplot(t_df1000, color='b');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WqsBLTz9hga"
      },
      "source": [
        "i = 10\n",
        "for sample in [t_df10, t_df100, t_df1000]:\n",
        "    print(f\"t - distribution with {i} degrees of freedom\")\n",
        "    print(\"---\" * 10)\n",
        "    print(f\"Mean: {sample.mean()}\")\n",
        "    print(f\"Standard Deviation: {sample.std()}\")\n",
        "    print(f\"Variance: {sample.var()}\")\n",
        "    i = i*10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDzbAgvF1DTB"
      },
      "source": [
        "## T-Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uax7iI16CWp1"
      },
      "source": [
        "# one sample t-test\n",
        "# for one sample t test, you are never putting in the null hypothesis.\n",
        "\n",
        "# for a null hypothsis is always that they are the same:\n",
        "\n",
        "# Run 1-sample t-test providing sample and null hypothesis\n",
        "# pass nan_policy='omit' any time you nave NaN values in a column\n",
        "# the second number (the mean-pop:, set to zero in the example) is the hypothesis\n",
        "ttest_1samp(rep['budget'], 0, nan_policy='omit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0ash6BK9yaR"
      },
      "source": [
        "###1 Sample T-Test Results:\n",
        "Concluding Remarks:\n",
        "\n",
        "Given the results of the above test I would REJECT the null hypothesis that there is no Democrat support for the handicapped-infants bill at the 95% significance level.\n",
        "\n",
        "\n",
        "In Layman's terms \n",
        "\n",
        "It would be a FALSE statment to declare that there is no democratic support for this bill. That's something that you might hear a political pundit declare, but you'll notice that they don't report their alpha value or p-value when they make such claims, they just spew them. --Tell us how you really feel!\n",
        "\n",
        "\n",
        "\n",
        "Based on a t-statistic of 19.8 and a p-value of 0, we reject the null hypothesis that there is 0 Democrat support for the Handicapped Infants bill, and suggest the alternative, that Democrat support is non-zero.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmLqhmSdCWst"
      },
      "source": [
        "#two sample t-test\n",
        "print(ttest_ind(rep['water-project'], dem['water-project'], nan_policy='omit'))\n",
        "print(rep['water-project'].mean())\n",
        "print(dem['water-project'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZBoepUs3NpO"
      },
      "source": [
        "###2 Sample T-Test results\n",
        "\n",
        "Concluding Remarks:\n",
        "\n",
        "(Formal Concluding Remarks)\n",
        "Due to our test resulting in a T-Statistic of 9.737575825219457 and a P-value of 2.3936722520597287e-20: we reject the null hypothesis that the two vote means are not different.\n",
        "\n",
        "\n",
        "(Explanatory/Laymans Terms)\n",
        "Here the small pvalue below .05, indicates a lack of similarity between the two means compared. The null hypothesis is that they are NOT different, and the result showing that they ARE different means strongly rejecting (the null hypothesis) that they are not different. The Large T value shows a larger difference, and the small P-value indicates an extremely low probability that the apparent difference appeared by chance alone. \n",
        "\n",
        "\n",
        "\n",
        "Ttest_indResult\n",
        "\n",
        "(statistic=9.737575825219457, \n",
        "\n",
        "pvalue=2.3936722520597287e-20) \n",
        "\n",
        "0.8975903614457831 Republican mean \n",
        "\n",
        "0.47674418604651164 Democratic mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUEwOGVfCWnU"
      },
      "source": [
        "def conf_int(data, confidence=0.95):\n",
        "\n",
        "  data = np.array(data)\n",
        "  mean = np.mean(data)\n",
        "  n = len(data)\n",
        "  # stderr = stats.sem(data)\n",
        "  stderr = np.std(data, ddof=1) / np.sqrt(n)\n",
        "  margin_of_error = stderr * stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
        "  print(margin_of_error)\n",
        "  return (mean, mean - margin_of_error, mean + margin_of_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucRnwf-uPPUR"
      },
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "  \"\"\"\n",
        "  Calculate a confidence interval around a sample mean for given data.\n",
        "  Using t-distribution and two-tailed test, default 95% confidence. \n",
        "  \n",
        "  Arguments:\n",
        "    data - iterable (list or numpy array) of sample observations\n",
        "    confidence - level of confidence for the interval\n",
        "  \n",
        "  Returns:\n",
        "    tuple of (mean, lower bound, upper bound)\n",
        "  \"\"\"\n",
        "  data = np.array(data)\n",
        "  mean = np.mean(data)\n",
        "  n = len(data)\n",
        "  # stderr = stats.sem(data)\n",
        "  stderr = np.std(data, ddof=1) / np.sqrt(n)\n",
        "  margin_of_error = stderr * stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
        "  print(margin_of_error)\n",
        "  return (mean, mean - margin_of_error, mean + margin_of_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz_BI1OCx_Ku"
      },
      "source": [
        "#Bayesian function\n",
        "# \"alpha\" is the confidence interval\n",
        "mean, variance, stdev = stats.bayes_mvs(dem['water-project'].dropna(),alpha=.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrxMB42xx_GJ"
      },
      "source": [
        "def conf_int(data, confidence=0.95):\n",
        "\n",
        "  data = np.array(data)\n",
        "  mean = np.mean(data)\n",
        "  n = len(data)\n",
        "  stderr = np.std(data, ddof=1) / np.sqrt(n)\n",
        "  margin_of_error = stderr * stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
        "  print(margin_of_error)\n",
        "\n",
        "  return ('The Mean =' mean, 'Mean - Margin of Error =' mean - margin_of_error, 'Mean + Margin of Error=' mean + margin_of_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUptOQKmpv0v"
      },
      "source": [
        "# mean, variance, standard deviation\n",
        "#\n",
        "#the values in question\n",
        "sales = [3505, 2400, 3027, 2798, 3700, 3250, 2689]\n",
        "\n",
        "#defining a function to calculate mean, variance, and standard deviation\n",
        "def list_stdev(list):\n",
        "  #creating a new list to fill later with calculated values\n",
        "  mean_diff = []\n",
        "  #this calculates the mean (sum divided by list length)\n",
        "  list_mean = sum(list)/len(list)\n",
        "  print (\"The mean('average') is:\", list_mean)\n",
        "  #this creates a new list of differneces between each value and the mean\n",
        "  #append adds the results of cycling i through the list\n",
        "  for i in sales:\n",
        "    mean_diff.append((i - list_mean)**2)\n",
        "  #this calculates variance: the mean of those differences (between each value and the mean)\n",
        "  list_variance = sum(mean_diff)/len(mean_diff)\n",
        "  print (\"The variance is:\", list_variance)\n",
        "  #this printes out the standard deviation, which is the square root (or X^.5) of the variance\n",
        "  print (\"The standard deviation is:\", list_variance**.5)\n",
        "  #this ends the programe\n",
        "  return \n",
        "\n",
        "list_stdev(sales)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIRxcCK1sFTh"
      },
      "source": [
        "# variance, standard deviation\n",
        "def variance(numbers):\n",
        " meanval = mean(numbers)\n",
        " # print(meanval)\n",
        " lenght = len(numbers)\n",
        " sumAll=0.0\n",
        " for a in numbers:\n",
        "   # print(a)\n",
        "   x = a-meanval #Xi-Xm\n",
        "   y = x**2\n",
        "   sumAll = sumAll+y\n",
        " return(sumAll/lenght)\n",
        "print(variance(values))\n",
        "def std(numbers):\n",
        "   return (variance(numbers)) ** (1/2)\n",
        "print(std(values))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pefjjSu-yAJa"
      },
      "source": [
        "## Exporting Files and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPUsDw5np3z4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE-Ihr4Lx_Cz"
      },
      "source": [
        "#loads into df\n",
        "#(broken) df = pd.DateFrame(data)\n",
        "df = pd.read_csv('file_name')\n",
        "#exports df (current working directory)\n",
        "df.to_csv('data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLAprjIihGx7"
      },
      "source": [
        "df.to_json(r'Path where you want to store the exported JSON file\\File Name.json')\n",
        "posts.to_json(r'Path where you want to store the exported JSON file\\posts_json.json')\n",
        "\n",
        "See:\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65bgZp9_DW9Q"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oRfAFm0DBih"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YlCa_PyDQpm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9yHXty-DXLo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQPliNFQDQ-T"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnXOv_NSkUw0"
      },
      "source": [
        "# Predict on test\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krMb8tPOvtlx"
      },
      "source": [
        "sample_submission['status_group'] = pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdYZJsnAltpk"
      },
      "source": [
        "submission['status_group'] = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt6FzlPmtNY_"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOVlYJr2vzvI"
      },
      "source": [
        "\n",
        "submission.to_csv('submission_GGA3.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OypvA7CQwAFR"
      },
      "source": [
        "#df.to_csv(file_name\n",
        "df.to_csv('file_name.csv')\n",
        "\n",
        "# download to local computer get csv\n",
        "from google.colab import files\n",
        "files.download('file_name.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAPCSRlMgt14"
      },
      "source": [
        "## Adults dataset\n",
        "\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Adult"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rbk9zcKgtJB"
      },
      "source": [
        "dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "column_headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
        "                 'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
        "                 'capital-gain', 'capital-loss', 'hours-per-week', \n",
        "                 'native-country', 'income']\n",
        "\n",
        "df = pd.read_csv(dataset_url, names=column_headers)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oPHzZZMJI98"
      },
      "source": [
        "#Pipeline example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kADTxw5GJKiU"
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(strategy='mean'), \n",
        "    StandardScaler(), \n",
        "    LogisticRegression(multi_class='auto', solver='lbfgs', n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Score on val\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
        "\n",
        "# Predict on test\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a-Z2FrWhfFp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GMq_BLphfP3"
      },
      "source": [
        "# confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ4zkc44inya"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqL-RfCVgLdk"
      },
      "source": [
        "##Accuracy: \n",
        "(correct predictions) / (sum of whole matrix \"total predictions\")\n",
        "\n",
        "accuracy_score(y_val, y_pred)\n",
        "\n",
        "##Precision: \n",
        "precision = correct (for one class)/predicted(one class)\n",
        "\n",
        "\n",
        "##Recall: \n",
        "recall = correct / actual (for one class only)\n",
        "\n",
        "\n",
        "### classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        " precision    recall  f1-score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MSWehj9n_iO"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#confusion_matrix#? #? gives help pane"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoDtpWL2dLRc"
      },
      "source": [
        "array([[7005,  171,  622],\n",
        "       [ 555,  332,  156],\n",
        "       [1098,   68, 4351]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdfSvrEpdrol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "dbe2c14c-5117-4311-aa9b-b90e02c0d054"
      },
      "source": [
        "#labels\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "unique_labels(y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['functional', 'functional needs repair', 'non functional'],\n",
              "      dtype='<U23')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cfpwOAribsF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su4dM9xTfdBa"
      },
      "source": [
        "# This is hard to read\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_val, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ztclSXDdHNE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a513049b-2144-4f82-ca58-100d6419ea2e"
      },
      "source": [
        "# We need to get labels\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "unique_labels(y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['functional', 'functional needs repair', 'non functional'],\n",
              "      dtype='<U23')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD_qvgCVhf3w"
      },
      "source": [
        "# 3. Plot a heatmap\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    labels = unique_labels(y_true)\n",
        "    columns = [f'Predicted {label}' for label in labels]\n",
        "    index = [f'Actual {label}' for label in labels]\n",
        "    table = pd.DataFrame(confusion_matrix(y_true, y_pred), \n",
        "                         columns=columns, index=index)\n",
        "    #return sns.heatmap(table)\n",
        "    return sns.heatmap(table, annot=True, fmt='d', cmap='viridis')\n",
        "\n",
        "plot_confusion_matrix(y_val, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qh3wCq_iL5o"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-US5jfc4jJd_"
      },
      "source": [
        "# regression outside of pipeline example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_36gWyyvFTB7"
      },
      "source": [
        "# Take 2: Using SK Learn Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYKz6d1LFTB-"
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiy1qrVOFTCA"
      },
      "source": [
        "#setting the X Y targets for logistic regression\n",
        "#X_train_logistic = X_train_imputed\n",
        "#y_train_logistic = train_labels['status_group']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nClusioDFTCB"
      },
      "source": [
        "my_train = train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR_CVhFpFTCD"
      },
      "source": [
        "target = train_labels['status_group']\n",
        "features = ['gps_height', 'longitude', 'latitude', 'region_code']\n",
        "X_train = train_features[features]\n",
        "y_train = my_train[target]\n",
        "X_val = my_val[features]\n",
        "y_val = my_val[target]\n",
        "\n",
        "imputer = SimpleImputer()\n",
        "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
        "X_val_imputed = imputer.transform(X_val_encoded)\n",
        "\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7OPIQ0bFTCE"
      },
      "source": [
        "model = LogisticRegressionCV(cv=5, n_jobs=-1, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "model.score(X_val_scaled, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJsBmE_NFTCG"
      },
      "source": [
        "%matplotlib inline\n",
        "coefficients = pd.Series(model.coef_[0], X_train_encoded.columns)\n",
        "coefficients.sort_values().plot.barh();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWxOD30rFTCI"
      },
      "source": [
        "X_train_encoded.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZwetvOCFTCJ"
      },
      "source": [
        "X_test = test[features]\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "X_test_imputed = imputer.transform(X_test_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1bADOk4mXX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liNqPrcj4nAV"
      },
      "source": [
        "random numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV3x8tZn4nrP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Blwd145rKTb"
      },
      "source": [
        "#### Use NumPy to generate a random number between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1kAyoIF3ZAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49707425-f171-4148-ce1f-4aea9caaf3cd"
      },
      "source": [
        "random.random()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1905308578592998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf_RdBOfrKTc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13852b39-f4a0-45c9-9aa2-cad41818b0e5"
      },
      "source": [
        "np.random.rand(1)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9092240707949499"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eef9Z3WU2qMs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3d69b2f-0591-4df1-fa3f-7af1abd60607"
      },
      "source": [
        "#import random\n",
        "random.uniform(0,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22476292646891505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhzGH89T3Nlu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7ef2780-ea64-4474-f533-51eba387f1c0"
      },
      "source": [
        "import os\n",
        "int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34866504594732506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LL3ea_44Tx0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef6b0476-7e83-4a46-f4e3-b856986015cc"
      },
      "source": [
        "#import random\n",
        "for i in range(1):\n",
        "    print(random.random())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8824447531063299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZknqNQ4P4Asu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3e704fc1-3eb6-4a9a-f264-cb3b7e4d9345"
      },
      "source": [
        "#import random\n",
        "for i in range(10):\n",
        "    print(random.random())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8300609632228523\n",
            "0.7309707703448267\n",
            "0.34169306961325374\n",
            "0.40587457695863083\n",
            "0.8307540588362339\n",
            "0.1494366527359493\n",
            "0.4679897811210033\n",
            "0.4502189814097758\n",
            "0.24322438000385904\n",
            "0.44183754942196707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5JplQgZrh_t"
      },
      "source": [
        "# yield vs. return changes the datastructure for functions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7P04DQnrf7K"
      },
      "source": [
        "# Random Section \n",
        "## Random generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_HtpZW0rjtK"
      },
      "source": [
        "# create a randomly populated dataframe\n",
        "\n",
        "# import pandas as pd\n",
        "# from numpy.random import randn\n",
        "df = pd.DataFrame(randn(5,4),['A','B','C','D','E'],['W','X','Y','z'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqYLUaG_sxBv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5akPoc0rsyI5"
      },
      "source": [
        "#### Use NumPy to generate a random number between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myJqUvCOsyI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49707425-f171-4148-ce1f-4aea9caaf3cd"
      },
      "source": [
        "random.random()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1905308578592998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeMbUYpluB55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63a242a9-73b6-4174-ced5-4576b1124a74"
      },
      "source": [
        "np.random.rand(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.58848872])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEOc-Q_WsyI9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13852b39-f4a0-45c9-9aa2-cad41818b0e5"
      },
      "source": [
        "np.random.rand(1)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9092240707949499"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jPVMpFYsyI-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3d69b2f-0591-4df1-fa3f-7af1abd60607"
      },
      "source": [
        "#import random\n",
        "random.uniform(0,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22476292646891505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBc6rgAsyJA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7ef2780-ea64-4474-f533-51eba387f1c0"
      },
      "source": [
        "import os\n",
        "int.from_bytes(os.urandom(8), byteorder=\"big\") / ((1 << 64) - 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34866504594732506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4avb3FTzsyJB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef6b0476-7e83-4a46-f4e3-b856986015cc"
      },
      "source": [
        "#import random\n",
        "for i in range(1):\n",
        "    print(random.random())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8824447531063299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5ld6IBksyJC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3e704fc1-3eb6-4a9a-f264-cb3b7e4d9345"
      },
      "source": [
        "#import random\n",
        "for i in range(10):\n",
        "    print(random.random())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8300609632228523\n",
            "0.7309707703448267\n",
            "0.34169306961325374\n",
            "0.40587457695863083\n",
            "0.8307540588362339\n",
            "0.1494366527359493\n",
            "0.4679897811210033\n",
            "0.4502189814097758\n",
            "0.24322438000385904\n",
            "0.44183754942196707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDAWqoFrrKTe"
      },
      "source": [
        "#### Use NumPy to generate an array of 25 random numbers sampled from a standard normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bC8QxcnrKTf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "46a459e8-5e14-4c92-8650-e7166e734bca"
      },
      "source": [
        "np.random.rand(5,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1580738 , 0.5334466 , 0.11239062, 0.59320151, 0.42296929],\n",
              "       [0.89235932, 0.82980556, 0.73412403, 0.90999092, 0.48176504],\n",
              "       [0.25152477, 0.4503155 , 0.70138715, 0.11458333, 0.43609971],\n",
              "       [0.30074596, 0.32054328, 0.16568624, 0.82693096, 0.26963493],\n",
              "       [0.55113328, 0.36021675, 0.42909119, 0.85727333, 0.4436998 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSmGipuy1u6I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b0899b01-00b6-4d41-c964-6905b7ab8c5d"
      },
      "source": [
        "np.random.rand(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.93815463, 0.68543331, 0.86309139, 0.03959841, 0.65358244,\n",
              "       0.48893579, 0.18278211, 0.59607266, 0.05311989, 0.01237394,\n",
              "       0.35403221, 0.30667484, 0.49378716, 0.68891036, 0.5543107 ,\n",
              "       0.13101884, 0.25462533, 0.39919792, 0.25978075, 0.89218448,\n",
              "       0.67931897, 0.47605744, 0.92871045, 0.13009352, 0.82562917])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfmTjvXfrkmp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u_bw3AdFgAM"
      },
      "source": [
        "## SQL\n",
        "\n",
        "making an sqlite database in memory "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRBAWvxWFglQ"
      },
      "source": [
        "from sqlalchemy import create_engine\n",
        "engine = create_engine('sqlite:///:memory:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrgrTxn7Yitg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MW6lDcBYjT2"
      },
      "source": [
        "# pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XEdWJg9YkxA"
      },
      "source": [
        "# Create multiple KNN models and pickle for use in the plotly dash app.\n",
        "for k in [5,10,15,20,25]:\n",
        "    mymodel = KNeighborsClassifier(n_neighbors=k, weights='distance', metric='euclidean')\n",
        "    mymodel.fit(X_train, y_train)\n",
        "    y_pred = mymodel.predict(X_test)\n",
        "    file = open(f'resources/model_k{k}.pkl', 'wb')\n",
        "    # dump my model into the file specified\n",
        "    pickle.dump(mymodel, file)\n",
        "    file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD_-EqM3q6Aj"
      },
      "source": [
        "def display_results(k, value0, value1):\n",
        "    # this opens the pickle\n",
        "    # the opposite of pickling the file\n",
        "    file = open(f'resources/model_k{k}.pkl', 'rb')\n",
        "    model=pickle.load(file)\n",
        "    file.close\n",
        "    new_obs=[[value0,value1]]\n",
        "    pred=model.predict(new_obs)\n",
        "    specieslist=['setosa', 'versicolor','verginica']\n",
        "    final_pred=specieslist[pred[0]]\n",
        "    return f'For a flower with sepal length {value0} and petal length {value1}, the predicted species is\"{final_pred}\"'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q8SKlK9Qkgs"
      },
      "source": [
        "# endpoints and search:\n",
        "\n",
        "#max-david-gg version 2 code: with total lenth\n",
        "\n",
        "# for the pipenv\n",
        "# basilica==0.2.8\n",
        "# certifi==2019.11.28\n",
        "# chardet==3.0.4\n",
        "# Click==7.0\n",
        "# Flask==1.1.1\n",
        "# gunicorn==20.0.4\n",
        "# idna==2.8\n",
        "# itsdangerous==1.1.0\n",
        "# Jinja2==2.10.3\n",
        "# MarkupSafe==1.1.1\n",
        "# numpy==1.18.1\n",
        "# pandas==0.25.3\n",
        "# Pillow==7.0.0\n",
        "# python-dateutil==2.8.1\n",
        "# python-dotenv==0.10.3\n",
        "# pytz==2019.3\n",
        "# requests==2.22.0\n",
        "# scipy==1.4.1\n",
        "# six==1.13.0\n",
        "# urllib3==1.25.7\n",
        "# Werkzeug==0.16.0\n",
        "# Flask-Cors\n",
        "\n",
        "# origin testing sandbox for this code\n",
        "# https://colab.research.google.com/drive/1lo-vG3jIOGl6_oepSTDuO2Ti9OeZuqda\n",
        "\n",
        "# Explanation for WEB:\n",
        "# Original dummy endpoint is now here at lambda AWS host:\n",
        "# http://3.12.71.81/dummy_data\n",
        "# New offset+range search:\n",
        "# http://3.12.71.81/offset_search\n",
        "# Description of: /offset_search route\n",
        "# expecting a json input like this:\n",
        "# {\"search\":\"AL FAWWAZ\", \"offset\":2, \"range\": 3}\n",
        "# This endpoint will return the rows starting after 'offset' and 'including 'range' number of results\n",
        "# This \"search\" (for this version) searches only the last name column: so names such as \"AL FAWWAZ\"\n",
        "# will return multiple result/rows that you can play with. These also have a multiple mentions:\n",
        "# - AL-FAUWAZ\n",
        "# - AL FAWWAZ\n",
        "# - ESSABAR\n",
        "# - AL-FAWWAZ\n",
        "# This does not do anything more than this for now, but improvements will come (including better search and total number of possible results, etc....and anything else you want use to scrabble into it!)\n",
        "# The exact formatting is not ideal, and doesn't include David's DOB improvements,but those will be updated/fixed. Hopefully this will give you enough to work with just to practice\n",
        "# This is an endpoint-tester than anyone can use to experiment with the new search:\n",
        "# https://drive.google.com/file/d/1bEWgKQk9Njqz9QyeKYrZsA6O6lStAFnc/view?usp=sharing (edited)\n",
        "\n",
        "from flask import Flask, jsonify, request\n",
        "from flask_cors import CORS\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import spatial\n",
        "\n",
        "#from vars_funcs import*\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "dummy_data = {\n",
        "\"unique_ID\": 365734763476,\n",
        "\"source_of_sanction\": \"USA\",\n",
        "\"map_place_locations\": \"this_may_be_a_few_different_type_collumns\",\n",
        "\"first_name\": \"fake_value_goes_here\",\n",
        "\"last_name\": \"fake_value_goes_here\",\n",
        "\"address\": \"fake_value_goes_here\",\n",
        "\"title\": \"fake_value_goes_here\",\n",
        "\"dob\": \"fake_value_goes_here\",\n",
        "\"pob\": \"fake_value_goes_here\",\n",
        "\"nationality\": \"fake_value_goes_here\",\n",
        "\"gender\": \"fake_value_goes_here\",\n",
        "\"passport\": \"fake_value_goes_here\",\n",
        "\"identification_number\": \"fake_value_goes_here\",\n",
        "\"national_id_no\": \"fake_value_goes_here\",\n",
        "\"company number\": \"fake_value_goes_here\",\n",
        "\"curp\": \"fake_value_goes_here\",\n",
        "\"website\": \"fake_value_goes_here\",\n",
        "\"email_address\": \"fake_value_goes_here\",\n",
        "\"tax_id_no\": \"fake_value_goes_here\",\n",
        "\"registration_id\": \"fake_value_goes_here\",\n",
        "\"swift/bic\": \"fake_value_goes_here\",\n",
        "\"government_gazette_number\": \"fake_value_goes_here\",\n",
        "\"c/o\": \"fake_value_goes_here\",\n",
        "\"personal_id_card\": \"fake_value_goes_here\",\n",
        "\"citizen\": \"fake_value_goes_here\",\n",
        "\"linked_to\": \"fake_value_goes_here\",\n",
        "\"cedula_no\": \"fake_value_goes_here\",\n",
        "\"company_number\": \"fake_value_goes_here\",\n",
        "\"certificate_of_incorporation_number\": \"fake_value_goes_here\",\n",
        "\"rfc\": \"fake_value_goes_here\",\n",
        "\"fka\": \"fake_value_goes_here\",\n",
        "\"aka\": \"fake_value_goes_here\",\n",
        "\"nka\": \"fake_value_goes_here\",\n",
        "\"nit\": \"fake_value_goes_here\",\n",
        "\"ssn\": \"fake_value_goes_here\",\n",
        "\"mmsi\": \"fake_value_goes_here\",\n",
        "\"entity\": \"fake_value_goes_here\",\n",
        "\"programs\": \"fake_value_goes_here\",\n",
        "\"date\": \"fake_value_goes_here\",\n",
        "\"action\": \"fake_value_goes_here\"\n",
        "}\n",
        "\n",
        "dummy_data_second = {\"results\":\n",
        "[{\"last_name\": \"(Linked To: GORRIN BELISARIO\", \"first_name\": None, \"address\": \". \", \"title\": None, \"dob\": None, \"pob\": None, \"nationality\": None, \"gender\": None, \"passport\": None, \"identification_number\": None, \"national_id_no\": None, \"company number\": None, \"curp\": None, \"website\": None, \"email_address\": None, \"tax_id_no\": None, \"registration_id\": None, \"swift_bic\": None, \"government_gazette_number\": None, \"c_o\": None, \"personal_id_card\": None, \"citizen\": None, \"linked_to\": \"GORRIN BELISARIO, Raul\", \"cedula_no\": None, \"company_number\": None, \"certificate_of_incorporation_number\": None, \"rfc\": None, \"fka\": None, \"aka\": None, \"nka\": None, \"nit\": None, \"ssn\": None, \"mmsi\": None, \"entity\": None, \"programs\": None, \"action\": \"added\", \"year\": 2019, \"month\": 1, \"day\": 8}, {\"last_name\": \" DE PERDOMO\", \"first_name\": None, \"address\": \"144 Isla Dorada Blvd., Coral Gables, FL 33146, United States;4100 Salzedo Street, Apt 1010, Miami, FL 33146, United States\", \"title\": None, \"dob\": \"25 Mar 1972\", \"pob\": None, \"nationality\": None, \"gender\": \"Female\", \"passport\": \"135278046 Venezuela expires 14 Oct 2020;079280833 Venezuela expires 22 Oct 2018;018516885 Venezuela expires 04 Dec 2013  \", \"identification_number\": None, \"national_id_no\": None, \"company number\": None, \"curp\": None, \"website\": None, \"email_address\": None, \"tax_id_no\": None, \"registration_id\": None, \"swift_bic\": None, \"government_gazette_number\": None, \"c_o\": None, \"personal_id_card\": None, \"citizen\": \"Venezuela\", \"linked_to\": None, \"cedula_no\": \"10538067 Venezuela\", \"company_number\": None, \"certificate_of_incorporation_number\": None, \"rfc\": None, \"fka\": None, \"aka\": \"DE PERDOMO, Maria A;PERDOMO ROSALES, Maria Alexandra;PERDOMO, Maria Alexandra;PERDOMO-ROSALES, Maria\", \"nka\": None, \"nit\": None, \"ssn\": None, \"mmsi\": None, \"entity\": \"individual\", \"programs\": \"[VENEZUELA-EO13850] \", \"action\": \"added\", \"year\": 2019, \"month\": 1, \"day\": 8}, {\"last_name\": \"(Linked To: GORRIN BELISARIO\", \"first_name\": None, \"address\": \". \", \"title\": None, \"dob\": None, \"pob\": None, \"nationality\": None, \"gender\": None, \"passport\": None, \"identification_number\": None, \"national_id_no\": None, \"company number\": None, \"curp\": None, \"website\": None, \"email_address\": None, \"tax_id_no\": None, \"registration_id\": None, \"swift_bic\": None, \"government_gazette_number\": None, \"c_o\": None, \"personal_id_card\": None, \"citizen\": None, \"linked_to\": \"GORRIN BELISARIO, Raul\", \"cedula_no\": None, \"company_number\": None, \"certificate_of_incorporation_number\": None, \"rfc\": None, \"fka\": None, \"aka\": None, \"nka\": None, \"nit\": None, \"ssn\": None, \"mmsi\": None, \"entity\": None, \"programs\": None, \"action\": \"added\", \"year\": 2019, \"month\": 1, \"day\": 8}]}\n",
        "\n",
        "\n",
        "# sample data\n",
        "#df = pd.read_csv('SDN02.csv', encoding='unicode_escape')\n",
        "df = pd.read_csv('SDNALL.csv', encoding='unicode_escape')\n",
        "\n",
        "# Need to install key manager package\n",
        "# Need to install git secrets\n",
        "# Need to insert AWS keys\n",
        "#app.config['SECRET_KEY'] = \"EMPTY\"\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"\"\"endpoints for beta Sanctions Explorer -- data science API home route\n",
        "    search expecting input as this format: {\"search\":\"AL FAWWAZ\", \"offset\":2, \"range\": 3 }\n",
        "    https://endpoints1.herokuapp.com/offset_search\n",
        "\n",
        "    example output (fixed)\n",
        "    https://endpoints1.herokuapp.com/dummy_data\n",
        "\n",
        "    Data Schema(term?)\n",
        "    https://endpoints1.herokuapp.com/dummy_data_schema\n",
        "\n",
        "    Maybe not working search and totals (but maybe working, sending 2 json objects)\n",
        "    https://endpoints1.herokuapp.com/offset_search_totals\n",
        "\n",
        "    Endpoint returning offset-range slice of results plus results total\n",
        "    https://endpoints1.herokuapp.com/offset_search_totals2\n",
        "\n",
        "    Differently formatted json dictionary search\n",
        "    https://endpoints1.herokuapp.com/offset_search_reformat\n",
        "\n",
        "    searchs all data offset range totals (slow)\n",
        "    https://endpoints1.herokuapp.com//offset_search_all_data_1\n",
        "\n",
        "    search to specify column is up and running!\n",
        "    input format\n",
        "    {\"feature_column\": 'last_name', \"search\":\"AL FAWWAZ\", \"offset\":2, \"range\": 3}\n",
        "    https://endpoints1.herokuapp.com/pick_column_offset_searchtotal\n",
        "\n",
        "    # searches partial string search\n",
        "    # form of input {\"search\":'AL FAWWAZ', \"offset\":2, \"range\": 3}\n",
        "    https://endpoints1.herokuapp.com/partial_match_search_all_columns1\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "@app.route('/dummy_data')\n",
        "def test_dummy_data():\n",
        "    return jsonify(dummy_data)\n",
        "\n",
        "@app.route('/dummy_data_schema')\n",
        "def test_dummy_data_schema():\n",
        "    return jsonify(dummy_data_second)\n",
        "\n",
        "@app.route('/offset_search', methods=['POST'])\n",
        "def offset_search_1():\n",
        "    #this is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this plugs the user's search term into the \"search\"\n",
        "    df_search = df[df['last_name'] == search1]\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_json(orient='index')\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return json_output\n",
        "\n",
        "@app.route('/offset_search_reformat', methods=['POST'])\n",
        "def offset_search_reformat1():\n",
        "    #this is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this plugs the user's search term into the \"search\"\n",
        "    df_search = df[df['last_name'] == search1]\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return json_output\n",
        "\n",
        "# new endpoint that includes the total possible search results\n",
        "@app.route('/offset_search_totals', methods=['POST'])\n",
        "def offset_search_2():\n",
        "    #this is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this plugs the user's search term into the \"search\"\n",
        "    df_search = df[df['last_name'] == search1]\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    #this creates a new json file with both jsons combined\n",
        "    output_with_total_search_result_number = {**search_lenth_dict, **json_output}\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return search_lenth_dict, json_output\n",
        "\n",
        "# new endpoint that includes the total possible search results\n",
        "@app.route('/offset_search_totals2', methods=['POST'])\n",
        "def offset_search_3():\n",
        "    #this is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this plugs the user's search term into the \"search\"\n",
        "    df_search = df[df['last_name'] == search1]\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    #this creates a new json file with both jsons combined\n",
        "    output_with_total_search_result_number = {\"total_number_of_possible_rows_returned\":search_length1,\"search results\": json_output}\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return output_with_total_search_result_number\n",
        "\n",
        "# new endpoint that includes the total possible search results\n",
        "@app.route('/offset_search_all_data_1', methods=['POST'])\n",
        "def offset_search_all_data1():\n",
        "    #this is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this plugs the user's search term into the \"search\"\n",
        "    #df_search = df[df['last_name'] == search1]\n",
        "    df_search = df[df.apply(lambda row: row.astype(str).str.contains(search1).any(), axis=1)]\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    #this creates a new json file with both jsons combined\n",
        "    output_with_total_search_result_number = {\"total_number_of_possible_rows_returned\":search_length1,\"search results\": json_output}\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return output_with_total_search_result_number\n",
        "\n",
        "# for specifying one column\n",
        "@app.route('/pick_column_offset_searchtotal', methods=['POST'])\n",
        "def pick_column_offset_searchtotal1():\n",
        "    # This is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    feature_column1 = json_obj[\"feature_column\"]\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this plugs the user's search term into the \"search\"\n",
        "    df_search = df[df[feature_column1] == search1]\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    #this creates a new json file with both jsons combined\n",
        "    output_with_total_search_result_number = {\"total_number_of_possible_rows_returned\":search_length1,\"search results\": json_output}\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return output_with_total_search_result_number\n",
        "\n",
        "# for specifying one column\n",
        "@app.route('/basic_literal_search_all_columns1', methods=['POST'])\n",
        "def basic_literal_search_all_columns_1():\n",
        "    # This is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    #feature_column1 = json_obj[\"feature_column\"]\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this searchs all collumns and merges the search_results\n",
        "\n",
        "    # This establishes a storage df outside of the loop\n",
        "    stored_df_search = pd.DataFrame()\n",
        "\n",
        "    # a for loop for doing a literal search of all colums for all data types\n",
        "    for column_name in df.columns:\n",
        "        # this searches each row, one row at a time for each\n",
        "        # for loop iteration\n",
        "        df_search = df[df[column_name] == search1]\n",
        "        # this merges/joins/contatenates/combines\n",
        "        #  the new search results into the cumulative\n",
        "        df_search = pd.concat([stored_df_search, df_search])\n",
        "        # this stores the cumulative date for the next loop iteration\n",
        "        stored_df_search = df_search\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    #this creates a new json file with both jsons combined\n",
        "    output_with_total_search_result_number = {\"total_number_of_possible_rows_returned\":search_length1,\"search results\": json_output}\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return output_with_total_search_result_number\n",
        "\n",
        "\n",
        "# for specifying one column\n",
        "@app.route('/partial_match_search_all_columns1', methods=['POST'])\n",
        "def partial_match_search_all_columns():\n",
        "    # This is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    #feature_column1 = json_obj[\"feature_column\"]\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "    # this searchs all collumns and merges the search_results\n",
        "\n",
        "    # This establishes a storage df outside of the loop\n",
        "    stored_df_search = pd.DataFrame()\n",
        "\n",
        "    # a for loop for doing a literal search of all colums for all data types\n",
        "    for column_name in df.columns:\n",
        "        # this searches each row, one row at a time for each\n",
        "        # for loop iteration\n",
        "        df_search = df[df[column_name].astype(str).str.contains(search1)]\n",
        "        # this merges/joins/contatenates/combines\n",
        "        #  the new search results into the cumulative\n",
        "        df_search = pd.concat([stored_df_search, df_search])\n",
        "        # this stores the cumulative date for the next loop iteration\n",
        "        stored_df_search = df_search\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    #search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    #this creates a new json file with both jsons combined\n",
        "    output_with_total_search_result_number = {\"total_number_of_possible_rows_returned\":search_length1,\"search results\": json_output}\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return output_with_total_search_result_number\n",
        "\n",
        "\n",
        "# for specifying one column\n",
        "# form of input: {\"search\":['SOGEDECOSA', 'THEARD, Jean-Luc'], \"offset\":2, \"range\": 3}\n",
        "@app.route('/multiple_search_terms__any_terms_cumulative1', methods=['POST'])\n",
        "def multiple_search_terms__any_terms_cumulative_1():\n",
        "    # This is the route the Frontend makes requests to\n",
        "\n",
        "    # this gets the json object\n",
        "    json_obj = request.get_json(force=True)\n",
        "\n",
        "    # these three lines brake up the json object into\n",
        "    # variables us-able by python\n",
        "    #feature_column1 = json_obj[\"feature_column\"]\n",
        "    search1 = json_obj[\"search\"]\n",
        "    offset1 = json_obj[\"offset\"]\n",
        "    range1 = json_obj[\"range\"]\n",
        "\n",
        "\n",
        "    # This establishes a storage df outside of the loop\n",
        "    stored_df_search = pd.DataFrame()\n",
        "    # this for loop loops through a list of search terms\n",
        "    for this_search_term in search1:\n",
        "\n",
        "        # a for loop for doing a literal search of all colums for all data types\n",
        "        for column_name in df.columns:\n",
        "            # this searches each row, one row at a time for each\n",
        "            # for loop iteration\n",
        "\n",
        "            df_search = df[df[column_name].astype(str).str.contains(this_search_term)]\n",
        "\n",
        "            # this merges/joins/contatenates/combines\n",
        "            #  the new search results into the cumulative\n",
        "            df_search = pd.concat([stored_df_search, df_search])\n",
        "            # this stores the cumulative date for the next loop iteration\n",
        "            stored_df_search = df_search\n",
        "\n",
        "    #this is how many total search returns there were\n",
        "    search_length1 = df_search.shape[0]\n",
        "\n",
        "    #this puts the total search returns into a json object/dictionary\n",
        "    #search_lenth_dict = {\"total_search_result_number\":search_length1}\n",
        "\n",
        "    # this applies the offset(where to start)\n",
        "    # and range (how many rows to display)\n",
        "    df_search = df_search.iloc[offset1:offset1 + range1]\n",
        "\n",
        "    # this turns output into a json object\n",
        "    json_output = df_search.to_dict(orient='index')\n",
        "\n",
        "    #this creates a new json file with both jsons combined\n",
        "    output_with_total_search_result_number = {\"total_number_of_possible_rows_returned\":search_length1,\"search results\": json_output}\n",
        "\n",
        "    # returns the search results per offset and range\n",
        "    return output_with_total_search_result_number\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    app.run(host=\"0.0.0.0\", port=80)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   app.run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaQPlzvTHJl"
      },
      "source": [
        "\n",
        " \"object spread operator\" or \"splatting\" in python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiq3mSpbTHdI"
      },
      "source": [
        "    # this uses an \"object spread operator\" or \"splatting\" in python\n",
        "    # trying to pre-set form and default empty values for summary_dictionary\n",
        "    # summary_dictionary['entity_type'] = {\"entity\":0, \"vessel\":0, \"eggs\":0}\n",
        "    # remake summary_dictionary['entity_type'], so all options are represented, with zero if empty\n",
        "    summary_dictionary['entity_type'] = df2_formatted[(df2_formatted['year'] == year1) & (df2_formatted['month'] == month1) ]['entity_type'].value_counts().to_dict()\n",
        "    remade_dictionary = {}\n",
        "    #remade_dictionary = {\"entity\": 0, \"individual\": 0, \"vessel\": 0, \"aircraft\": 0}\n",
        "    remade_dictionary = {\"entity\": 0, \"individual\": 0, \"vessel\": 0, \"aircraft\": 0, **summary_dictionary['entity_type']}\n",
        "    print(remade_dictionary)\n",
        "    summary_dictionary['entity_type'] = remade_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlDrLUE-u5cw"
      },
      "source": [
        "# dynamically set variable name\n",
        "\n",
        "# mask: getting x vector for name\n",
        "x_part_of_room_name = new_room_location[0]\n",
        "# mask: getting y vector for name\n",
        "y_part_of_room_name = new_room_location[1]\n",
        "new_room_description = vars()[f\"room{x_part_of_room_name}_{y_part_of_room_name}\"]\n",
        "print(f\"This room looks like {new_room_description}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCEfFzRHl6TJ"
      },
      "source": [
        "## making a variably sized array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b6_hZjkl6fp"
      },
      "source": [
        "    elements = len( arrA ) + len( arrB )\n",
        "    merged_arr = [0] * elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD_yToNLvATx"
      },
      "source": [
        "## permutations and enumerations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evd5VjibvAfL"
      },
      "source": [
        "# built in functions...not for assignment\n",
        "#https://www.codeproject.com/Tips/1275693/Recursive-Permutations-in-Python\n",
        "def permute(s):\n",
        "  out = []\n",
        "  if len(s) == 1:\n",
        "    return s\n",
        "  else:\n",
        "    for i,let in enumerate(s):\n",
        "      for perm in permute(s[:i] + s[i+1:]):\n",
        "        out += [let + perm]\n",
        "  return out\n",
        "  \n",
        "l = permute(['1','2','3'])\n",
        "l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4zBJeOoAqig"
      },
      "source": [
        "# Basic Sorting Functions\n",
        "\n",
        "https://colab.research.google.com/drive/1ao7W6_zJm1a9rcceeGi1X3kEj9SShBBX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEyHQeTaAuxf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASdnkfUmmD7E"
      },
      "source": [
        "timing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9HS3IgRmEJm"
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "f = open(\"names_1.txt\", \"r\")\n",
        "names_1 = f.read().split(\"\\n\")  # List containing 10000 names\n",
        "f.close()\n",
        "\n",
        "f = open(\"names_2.txt\", \"r\")\n",
        "names_2 = f.read().split(\"\\n\")  # List containing 10000 names\n",
        "f.close()\n",
        "\n",
        "duplicates = []  # Return the list of duplicates in this data structure\n",
        "\n",
        "# # Replace the nested for loops below with your improvements\n",
        "# for name_1 in names_1:\n",
        "#     for name_2 in names_2:\n",
        "#         if name_1 == name_2:\n",
        "#             duplicates.append(name_1)\n",
        "\n",
        "# get in list form the common elements between two sets\n",
        "duplicates = list(set(names_1) & set(names_2))\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"{len(duplicates)} duplicates:\\n\\n{', '.join(duplicates)}\\n\\n\")\n",
        "print(f\"runtime: {end_time - start_time} seconds\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1nX8oQ7rfoC"
      },
      "source": [
        "# lists section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_LbHozYriVr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9562fa5-a7a7-4de8-91f5-203f5ec32275"
      },
      "source": [
        "list_from_string = '''1\n",
        "2\n",
        "'etc'\n",
        "3'''.split('\\n')\n",
        "\n",
        "print(list_from_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '2', \"'etc'\", '3']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}